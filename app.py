import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import CountVectorizer
import torch
import re
import io

# =========================
# 0. é¡µé¢é…ç½®ä¸å®‰å…¨éªŒè¯
# =========================
st.set_page_config(page_title="AI è¯„è®ºåˆ†æ (ä¿®å¤ç‰ˆ)", page_icon="ğŸ”§", layout="wide")

ACCESS_PASSWORD = "admin123" 

if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def check_password():
    if st.session_state["password_input"] == ACCESS_PASSWORD:
        st.session_state.logged_in = True
    else:
        st.error("å¯†ç é”™è¯¯")

if not st.session_state.logged_in:
    st.markdown("## ğŸ”’ ç³»ç»Ÿé”å®š")
    st.text_input("è¯·è¾“å…¥è®¿é—®å¯†ç ", type="password", key="password_input", on_change=check_password)
    st.stop() 

# =========================
# 1. æ ‡ç­¾åº“ä¸å…³é”®è¯è§„åˆ™ (æ ¸å¿ƒä¿®å¤)
# =========================
# å®šä¹‰æ ‡ç­¾çš„åŒæ—¶ï¼Œå®šä¹‰â€œå¼ºåˆ¶å…³æ³¨è¯â€ã€‚å¦‚æœè¯„è®ºåŒ…å«è¿™äº›è¯ï¼ŒAI ä¼šåŠ å€å…³æ³¨å¯¹åº”æ ‡ç­¾ã€‚
POS_LABELS_MAP = {
    "é¢æ–™èˆ’é€‚/æŸ”è½¯": ["soft", "comfortable", "fabric", "material", "èˆ’é€‚", "è½¯", "é¢æ–™"],
    "åšå·¥è´¨é‡å¥½": ["quality", "well made", "sturdy", "è´¨é‡", "åšå·¥"],
    "ç¼“è§£ç–¼ç—›/åŒ»ç–—æ•ˆæœ": ["pain", "relief", "arthritis", "ache", "ç–¼ç—›", "ç¼“è§£", "å…³èŠ‚ç‚"],
    "å°ºç åˆèº«/èˆ’é€‚è´´åˆ": ["fit", "size", "snug", "perfect", "åˆèº«", "å°ºç "],
    "å¢åŠ æŠ“æ¡åŠ›/é˜²æ»‘": ["grip", "slip", "traction", "æŠ“æ¡", "æ»‘"],
    "è€ç”¨æ€§å¼º": ["durable", "last", "tear", "è€ç”¨", "ç ´"],
}

NEG_LABELS_MAP = {
    "å°ºç å¤ªå°/å¤ªç´§/ä¼¸ä¸è¿›å»": ["small", "tight", "fit", "cut off", "circulation", "cuff", "hand in", "ç´§", "å°", "å‹’", "ä¼¸ä¸è¿›", "çª„"],
    "å°ºç å¤ªå¤§/å¤ªæ¾": ["big", "loose", "huge", "large", "æ¾", "å¤§", "é•¿"],
    "æ— æ•ˆ/æ²¡æœ‰ä½œç”¨": ["work", "effect", "useless", "help", "æ— æ•ˆ", "æ²¡ç”¨"],
    "ç¼çº¿å¼€è£‚/ç ´æŸ": ["seam", "rip", "tear", "hole", "split", "ç¼çº¿", "ç ´", "æ´", "å¼€è£‚"],
    "é¢æ–™è´¨é‡å·®/å»‰ä»·": ["material", "thin", "cheap", "rough", "scratchy", "é¢æ–™", "è–„", "ç²—ç³™"],
    "å¤ªæ»‘/æ²¡æœ‰æŠ“æ¡åŠ›": ["slippery", "slide", "no grip", "smooth", "æ»‘", "æŠ“ä¸ä½"],
    "è¿‡æ•/çš®ç–¹/å‘ç—’": ["rash", "itch", "allergy", "skin", "ç—’", "è¿‡æ•", "çº¢è‚¿"]
}

# æå–çº¯æ ‡ç­¾åˆ—è¡¨ä¾›æ¨¡å‹ç¼–ç 
POS_LABELS = list(POS_LABELS_MAP.keys())
NEG_LABELS = list(NEG_LABELS_MAP.keys())

# =========================
# 2. AI æ¨¡å‹åŠ è½½
# =========================
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# =========================
# 3. æ ¸å¿ƒåŠŸèƒ½ï¼šæ··åˆæ‰“æ ‡ (å…³é”®è¯ + è¯­ä¹‰)
# =========================
def extract_dynamic_label(text, model, ngram_range=(2, 3)):
    """æå–æ–°æ ‡ç­¾"""
    try:
        is_chinese = bool(re.search(r'[\u4e00-\u9fff]', text))
        analyzer_type = 'char' if is_chinese else 'word'
        count = CountVectorizer(ngram_range=ngram_range, analyzer=analyzer_type, stop_words='english').fit([text])
        candidates = count.get_feature_names_out()
        if len(candidates) == 0: return "å…¶ä»–æœªåˆ†ç±»"
        doc_embedding = model.encode([text])
        candidate_embeddings = model.encode(candidates)
        distances = util.cos_sim(doc_embedding, candidate_embeddings)
        keywords = [candidates[index] for index in distances.argsort()[0][-1:]]
        tag = keywords[0]
        return tag.replace(" ", "") if is_chinese else tag.title()
    except:
        return "å…¶ä»–(æ–‡æœ¬è¿‡çŸ­)"

def hybrid_classify(df, model, match_threshold=0.35):
    """
    æ··åˆæ‰“æ ‡é€»è¾‘ï¼š
    1. å…³é”®è¯å¢å¼ºï¼šå¦‚æœè¯„è®ºå«æœ‰ "tight", "fit"ï¼Œä¼šç»™ "å°ºç " ç±»æ ‡ç­¾åŠ åˆ†ã€‚
    2. è¯­ä¹‰åŒ¹é…ï¼šä½¿ç”¨ AI è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ã€‚
    """
    reviews = df['text'].tolist()
    
    # 1. å‘é‡ç¼–ç 
    review_embeddings = model.encode(reviews, convert_to_tensor=True)
    pos_embeddings = model.encode(POS_LABELS, convert_to_tensor=True)
    neg_embeddings = model.encode(NEG_LABELS, convert_to_tensor=True)
    
    # 2. è®¡ç®—åŸå§‹ç›¸ä¼¼åº¦
    pos_sims = util.cos_sim(review_embeddings, pos_embeddings)
    neg_sims = util.cos_sim(review_embeddings, neg_embeddings)
    
    final_labels = []
    sentiment_display = []
    is_new_label = []

    progress_bar = st.progress(0)
    total = len(df)
    
    for i in range(total):
        if i % 10 == 0: progress_bar.progress(i / total)

        rating = df.iloc[i]['rating']
        text = str(df.iloc[i]['text']).lower()
        
        # --- å…³é”®è¯åŠ æƒ (Booster) ---
        # å¦‚æœè¯„è®ºé‡Œæœ‰ "tight"ï¼Œåˆ™ "å°ºç å¤ªå°" çš„ç›¸ä¼¼åº¦åˆ†æ•° +0.3
        
        # å¤„ç†å·®è¯„æƒé‡
        current_neg_scores = neg_sims[i].clone()
        for idx, label in enumerate(NEG_LABELS):
            keywords = NEG_LABELS_MAP[label]
            if any(k in text for k in keywords):
                current_neg_scores[idx] += 0.35  # æ˜¾è‘—æå‡åŒ…å«å…³é”®è¯çš„æ ‡ç­¾åˆ†æ•°

        # å¤„ç†å¥½è¯„æƒé‡
        current_pos_scores = pos_sims[i].clone()
        for idx, label in enumerate(POS_LABELS):
            keywords = POS_LABELS_MAP[label]
            if any(k in text for k in keywords):
                current_pos_scores[idx] += 0.35

        # è·å–åŠ æƒåçš„æœ€ä½³åŒ¹é…
        best_pos_idx = torch.argmax(current_pos_scores).item()
        best_pos_score = current_pos_scores[best_pos_idx].item()
        
        best_neg_idx = torch.argmax(current_neg_scores).item()
        best_neg_score = current_neg_scores[best_neg_idx].item()
        
        label = None
        s_display = "æœªçŸ¥"
        is_new = False
        
        # --- ä¸¥æ ¼çš„æƒ…æ„Ÿåˆ¤å®š ---
        # 3æ˜Ÿç»å¯¹æ˜¯å·®è¯„
        if rating <= 3:
            is_negative = True
        elif rating == 4:
            is_negative = best_neg_score > best_pos_score
        else:
            is_negative = False

        # --- æœ€ç»ˆå†³ç­– ---
        if is_negative:
            s_display = "å·®è¯„"
            if best_neg_score > match_threshold:
                label = NEG_LABELS[best_neg_idx]
            else:
                label = extract_dynamic_label(df.iloc[i]['text'], model)
                is_new = True
        else:
            s_display = "å¥½è¯„"
            if best_pos_score > match_threshold:
                label = POS_LABELS[best_pos_idx]
            else:
                label = extract_dynamic_label(df.iloc[i]['text'], model)
                is_new = True
        
        final_labels.append(label)
        sentiment_display.append(s_display)
        is_new_label.append(is_new)

    progress_bar.empty()
    return final_labels, sentiment_display, is_new_label

# =========================
# 4. è¾…åŠ©å·¥å…· (è¯„åˆ†ä¿®å¤)
# =========================
def load_file(f):
    if f.name.lower().endswith(".csv"):
        try: return pd.read_csv(f, encoding="utf-8")
        except: return pd.read_csv(f, encoding="gbk")
    return pd.read_excel(f)

def parse_rating_strict(x):
    """ä¸¥æ ¼è§£æè¯„åˆ†ï¼Œå¼ºåˆ¶è½¬ä¸º 1-5 çš„æ•´æ•°"""
    if pd.isna(x): return np.nan
    s = str(x)
    # æå–æ•°å­—
    m = re.search(r"(\d+(\.\d+)?)", s)
    if m:
        val = float(m.group(1))
        # å››èˆäº”å…¥å¹¶å–æ•´
        val_int = int(round(val))
        # è¾¹ç•Œä¿æŠ¤
        if val_int < 1: val_int = 1
        if val_int > 5: val_int = 5
        return val_int
    return np.nan

# =========================
# 5. ä¸»ç¨‹åº UI
# =========================
st.title("ğŸ“Š AI è¯„è®ºåˆ†æ (Hybrid å¢å¼ºç‰ˆ)")
st.markdown("""
**æœ¬æ¬¡æ›´æ–°ä¿®å¤ï¼š**
1. **è¯„åˆ†ä¿®æ­£**ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯„åˆ†ï¼ˆå¦‚ 3.0, 4.0ï¼‰è½¬ä¸ºæ•´æ•°ï¼Œå‡†ç¡®ç»Ÿè®¡ 3 æ˜Ÿå·®è¯„ã€‚
2. **æ‰“æ ‡ä¿®æ­£**ï¼šå¼•å…¥â€œå…³é”®è¯è§„åˆ™â€ï¼Œå½“è¯„è®ºæåˆ°â€œè¢–å£â€ã€â€œä¼¸ä¸è¿›â€æ—¶ï¼Œå¼ºåˆ¶åˆ¤å®šä¸ºã€å°ºç é—®é¢˜ã€‘ï¼Œä¸å†è¯¯åˆ¤ä¸ºæ»‘ã€‚
""")

with st.spinner("AI å¼•æ“å¯åŠ¨ä¸­..."):
    model = load_model()

uploaded = st.file_uploader("ä¸Šä¼ è¯„è®ºæ–‡ä»¶ (CSV/Excel)", type=["csv", "xlsx"])

if uploaded:
    with st.spinner('æ­£åœ¨æ¸…æ´—æ•°æ®å¹¶è¿›è¡Œæ··åˆåˆ†æ...'):
        df = load_file(uploaded)
        
        # 1. å­—æ®µè¯†åˆ«
        all_cols = df.columns.tolist()
        # å°è¯•æ‰¾ rating åˆ—ï¼Œå¦‚æœæ²¡æœ‰åŒ…å« "rating" æˆ– "æ˜Ÿ" çš„åˆ—ï¼Œé»˜è®¤ç”¨ç¬¬å‡ åˆ—
        rating_col_candidates = [c for c in all_cols if "æ˜Ÿ" in str(c) or "rating" in str(c).lower() or "score" in str(c).lower()]
        text_col_candidates = [c for c in all_cols if "å†…å®¹" in str(c) or "review" in str(c).lower() or "text" in str(c).lower() or "body" in str(c).lower()]
        
        rating_col = rating_col_candidates[0] if rating_col_candidates else all_cols[0]
        text_col = text_col_candidates[0] if text_col_candidates else all_cols[1]

        # 2. ä¸¥æ ¼æ¸…æ´—æ•°æ® (ä¿®å¤å›¾1çš„é—®é¢˜)
        # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°
        df["rating_clean"] = df[rating_col].apply(parse_rating_strict)
        # å»é™¤æ— æ•ˆè¯„åˆ†
        df = df.dropna(subset=["rating_clean"])
        df["rating_clean"] = df["rating_clean"].astype(int)
        
        df["text"] = df[text_col].astype(str).fillna("")
        
        # ä¸ºäº†åç»­ä»£ç å…¼å®¹ï¼Œå°† rating_clean æ˜ å°„å› rating
        df["rating"] = df["rating_clean"]
        
        # 3. æ ¸å¿ƒè¿ç®—
        labels, sentiments, is_new = hybrid_classify(df, model)
        df["æ ‡ç­¾"] = labels
        df["æƒ…æ„Ÿåˆ†ç±»"] = sentiments
        df["æ˜¯å¦æ–°æ ‡ç­¾"] = is_new
        
    st.success("âœ… åˆ†æå®Œæˆï¼")

    # =========================
    # A: å®è§‚æ¦‚è§ˆ (ä¿®å¤ç‰ˆ)
    # =========================
    st.markdown("---")
    st.header("1. å®è§‚æ•°æ®æ¦‚è§ˆ (å·²ä¿®å¤)")
    
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("è¯„è®ºæ€»æ•°", len(df))
    
    avg_score = df['rating'].mean()
    k2.metric("å¹³å‡è¯„åˆ†", f"{avg_score:.2f} â­")
    
    # ä¸¥æ ¼è®¡ç®— <=3 æ˜Ÿ
    neg_df = df[df['rating'] <= 3]
    neg_count = len(neg_df)
    neg_rate = (neg_count / len(df) * 100) if len(df) > 0 else 0
    
    k3.metric("å·®è¯„å æ¯” (<=3æ˜Ÿ)", f"{neg_rate:.1f}%", delta_color="inverse")
    k4.metric("æ–°æ ‡ç­¾æŒ–æ˜æ•°", sum(is_new))
    
    # æ˜Ÿçº§åˆ†å¸ƒå›¾ (ä¿®å¤ä¸ºç¦»æ•£æŸ±çŠ¶å›¾)
    st.subheader("è¯„åˆ†ç­‰çº§åˆ†å¸ƒ")
    # å¼ºåˆ¶ç»Ÿè®¡ 1-5 çš„æ¯ä¸€ä¸ªæ•°é‡ï¼Œå³ä½¿æ˜¯ 0 ä¹Ÿè¦æ˜¾ç¤º
    counts = df['rating'].value_counts().reindex([1,2,3,4,5], fill_value=0).reset_index()
    counts.columns = ["æ˜Ÿçº§", "æ•°é‡"]
    # å¼ºåˆ¶æ˜Ÿçº§ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ Plotly æŠŠå®ƒå½“è¿ç»­æ•°å­—ç”»
    counts["æ˜Ÿçº§"] = counts["æ˜Ÿçº§"].astype(str) + "æ˜Ÿ"
    
    fig_bar = px.bar(counts, x="æ˜Ÿçº§", y="æ•°é‡", text="æ•°é‡", color="æ•°é‡", color_continuous_scale="Blues")
    st.plotly_chart(fig_bar, use_container_width=True)

    # =========================
    # B: æ·±åº¦å¯è§†åŒ–
    # =========================
    st.markdown("---")
    st.header("2. æ ‡ç­¾æ·±åº¦åˆ†æ")
    
    c1, c2 = st.columns(2)
    with c1:
        st.caption("æƒ…æ„Ÿåˆ†å¸ƒç¯å½¢å›¾")
        s_counts = df["æƒ…æ„Ÿåˆ†ç±»"].value_counts().reset_index()
        s_counts.columns = ["æƒ…æ„Ÿ", "æ•°é‡"]
        fig_pie = px.pie(s_counts, values="æ•°é‡", names="æƒ…æ„Ÿ", hole=0.4, 
                         color="æƒ…æ„Ÿ", color_discrete_map={"å¥½è¯„":"#2ecc71", "å·®è¯„":"#e74c3c"})
        st.plotly_chart(fig_pie, use_container_width=True)
        
    with c2:
        st.caption("é—®é¢˜å±‚çº§æ—­æ—¥å›¾")
        # è¿‡æ»¤ä½é¢‘
        viz_df = df.copy()
        tc = viz_df["æ ‡ç­¾"].value_counts()
        viz_df["æ ‡ç­¾å±•ç¤º"] = viz_df["æ ‡ç­¾"].apply(lambda x: x if tc[x] > 0 else "å…¶ä»–")
        
        sun_df = viz_df.groupby(["æƒ…æ„Ÿåˆ†ç±»", "æ ‡ç­¾å±•ç¤º"]).size().reset_index(name="æ•°é‡")
        fig_sun = px.sunburst(sun_df, path=['æƒ…æ„Ÿåˆ†ç±»', 'æ ‡ç­¾å±•ç¤º'], values='æ•°é‡',
                              color='æƒ…æ„Ÿåˆ†ç±»', color_discrete_map={"å¥½è¯„":"#2ecc71", "å·®è¯„":"#e74c3c"})
        st.plotly_chart(fig_sun, use_container_width=True)

    # =========================
    # C: å·®è¯„åŸå£° (éªŒè¯ä¿®å¤ç»“æœ)
    # =========================
    st.markdown("---")
    st.header("3. å·®è¯„åŸå£°é€è§†")
    st.caption("è¯·æ£€æŸ¥ï¼š'å°ºç é—®é¢˜' æ˜¯å¦åŒ…å«äº†æŠ±æ€¨è¢–å£ç´§çš„è¯„è®º")
    
    if not neg_df.empty:
        # åªçœ‹å·®è¯„
        neg_issues = neg_df["æ ‡ç­¾"].value_counts().index.tolist()
        selected_issue = st.selectbox("é€‰æ‹©å·®è¯„æ ‡ç­¾æŸ¥çœ‹:", neg_issues)
        
        reviews = neg_df[neg_df["æ ‡ç­¾"] == selected_issue][["rating", "text"]]
        
        st.markdown(f"**æ ‡ç­¾ã€{selected_issue}ã€‘ä¸‹çš„è¯„è®º:**")
        for idx, row in reviews.iterrows():
            st.warning(f"[{row['rating']}æ˜Ÿ] {row['text']}")
    else:
        st.info("æ­å–œï¼Œå½“å‰æ•°æ®ä¸­æ²¡æœ‰ <=3 æ˜Ÿçš„å·®è¯„ã€‚")

    # =========================
    # ä¸‹è½½
    # =========================
    st.markdown("---")
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Result')
    
    st.download_button("â¬‡ï¸ ä¸‹è½½ Excel ç»“æœ", buffer.getvalue(), "fixed_analysis.xlsx", "application/vnd.ms-excel")
