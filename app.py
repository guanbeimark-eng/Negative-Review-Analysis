import streamlit as st
import pandas as pd
import json
import uuid
import re
import numpy as np

# ======================================================
# 0) åŸºç¡€é…ç½® & ç™»å½•
# ======================================================
st.set_page_config(
    page_title="Amazon è¯„è®ºæ‰“æ ‡ç³»ç»Ÿï¼ˆè‡ªåŠ¨æ˜ å°„+å†…ç½®è¯„ä»·åº“ï¼‰",
    page_icon="ğŸ·ï¸",
    layout="wide"
)

ACCESS_PASSWORD = "admin123"

if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def check_password():
    if st.session_state.get("password_input") == ACCESS_PASSWORD:
        st.session_state.logged_in = True
    else:
        st.error("å¯†ç é”™è¯¯")

if not st.session_state.logged_in:
    st.markdown("## ğŸ”’ ç³»ç»Ÿé”å®š")
    st.text_input("è®¿é—®å¯†ç ", type="password", key="password_input", on_change=check_password)
    st.stop()

# ======================================================
# 1) å†…ç½®è¯„ä»·åº“ï¼ˆé»˜è®¤ï¼šä½ ä¸éœ€è¦ä¸Šä¼ ï¼‰
#    ä½ åç»­å¯ä»¥æŠŠè¿™é‡Œæ›¿æ¢æˆä½ â€œæ–‡ä»¶è¯„ä»·åº“â€çš„æ­£å¼æ ‡ç­¾
# ======================================================
TAG_LIBRARY = {
    "positive": [
        "ä½©æˆ´èˆ’é€‚",
        "æ”¯æ’‘æ€§å¥½",
        "ç¼“è§£å…³èŠ‚ä¸é€‚",
        "å°ºå¯¸åˆé€‚",
        "è´¨é‡å¥½",
        "æ€§ä»·æ¯”é«˜",
        "æ•ˆæœæ˜æ˜¾",
        "ç‰©æµ/å‘è´§å¿«",
        "å¤–è§‚å¥½çœ‹"
    ],
    "negative": [
        "å°ºç åå°",
        "å°ºç åå¤§",
        "å°ºç ä¸ä¸€è‡´",
        "ä¸é€‚åˆç”·å£«",
        "ç©¿æˆ´å›°éš¾",
        "è´¨é‡å·®",
        "ä¸æè¿°ä¸ç¬¦",
        "ä¸èˆ’é€‚/å‹’æ‰‹",
        "æ°”å‘³/å¼‚å‘³",
        "è€ç”¨æ€§å·®/æ˜“ç ´"
    ]
}

# ======================================================
# 2) Session State åˆå§‹åŒ–
# ======================================================
defaults = {
    "raw_df": None,
    "main_df": None,          # æ¸…æ´—åä¸»è¡¨ï¼ˆå«rating_intã€sys_uuidç­‰ï¼‰
    "norm_df": None,          # å½’ä¸€åŒ–è¡¨ï¼ˆid/rating/textï¼‰
    "mapping_locked": False,

    "col_map": None,          # è‡ªåŠ¨è¯†åˆ«åˆ°çš„åˆ—æ˜ å°„
    "tag_config": {
        "pos": TAG_LIBRARY["positive"],
        "neg": TAG_LIBRARY["negative"],
        "all": TAG_LIBRARY["positive"] + TAG_LIBRARY["negative"]
    },

    "generated_batches": [],
    "merged_full_df": None,   # åŸå§‹å­—æ®µ+AI_Label åˆå¹¶åçš„å¯¼å‡ºè¡¨
}
for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v

# ======================================================
# 3) å·¥å…·å‡½æ•°
# ======================================================
def load_file(f):
    name = f.name.lower()
    if name.endswith(".csv"):
        try:
            return pd.read_csv(f, encoding="utf-8")
        except UnicodeDecodeError:
            return pd.read_csv(f, encoding="gbk")
    return pd.read_excel(f)

def parse_rating(x):
    """å…¼å®¹äºšé©¬é€Šå¸¸è§ratingæ ¼å¼ï¼š'4.0 out of 5 stars' / 'Rated 3' / '5' / 4.0"""
    if pd.isna(x):
        return np.nan
    s = str(x)
    m = re.search(r"(\d+(?:\.\d+)?)", s)
    if not m:
        return np.nan
    try:
        return float(m.group(1))
    except:
        return np.nan

def safe_parse_json(text):
    """æ”¯æŒä¸€æ¬¡ç²˜è´´å¤šæ®µJSONï¼ˆç”¨ç©ºè¡Œåˆ†éš”ï¼‰"""
    if not text:
        return None
    clean = text.replace("```json", "").replace("```", "").strip()
    if not clean:
        return None

    # å…ˆå°è¯•æ•´ä½“è§£æ
    try:
        return json.loads(clean)
    except:
        pass

    # å†å°è¯•æŒ‰ç©ºè¡Œæ‹†åˆ†
    parts = [p.strip() for p in clean.split("\n\n") if p.strip()]
    merged = []
    ok = False
    for p in parts:
        try:
            obj = json.loads(p)
            if isinstance(obj, list):
                merged.extend(obj)
                ok = True
        except:
            continue
    return merged if ok else None

def validate_label(label, allowed_set: set):
    if label is None:
        return ""
    lab = str(label).strip()
    return lab if lab in allowed_set else ""

# -------- è‡ªåŠ¨åˆ—æ˜ å°„ï¼šé¢„è®¾ç»„åˆï¼ˆä¸è®©ç”¨æˆ·ç‚¹ï¼‰ --------
COLUMN_CANDIDATES = {
    "rating": ["æ˜Ÿçº§", "rating", "Rating", "è¯„åˆ†", "Score"],
    "title": ["æ ‡é¢˜", "title", "Title", "headline", "summary"],
    "content": ["å†…å®¹", "content", "Content", "review", "Review", "è¯„è®ºå†…å®¹", "body", "text"],
    "translation": ["å†…å®¹(ç¿»è¯‘)", "ç¿»è¯‘", "translation", "Translated", "å†…å®¹ï¼ˆç¿»è¯‘ï¼‰"],
    "date": ["è¯„è®ºæ—¶é—´", "date", "Date", "review_date", "time", "æ—¶é—´", "è¯„è®ºæ—¥æœŸ"],
    "id": ["review_id", "id", "ID", "è¯„è®ºID", "uuid", "å”¯ä¸€ID"],
}

def auto_match_column(cols, candidates):
    # 1) å…ˆç²¾ç¡®åŒ¹é…
    for c in candidates:
        if c in cols:
            return c
    # 2) å†æ¨¡ç³ŠåŒ…å«åŒ¹é…ï¼ˆåˆ—åé‡ŒåŒ…å«å…³é”®è¯ï¼‰
    lower_map = {c.lower(): c for c in cols}
    for cand in candidates:
        cand_l = cand.lower()
        for col in cols:
            if cand_l in col.lower():
                return col
    return None

def auto_build_mapping(df: pd.DataFrame):
    cols = df.columns.tolist()
    col_rating = auto_match_column(cols, COLUMN_CANDIDATES["rating"])
    col_title = auto_match_column(cols, COLUMN_CANDIDATES["title"])
    col_content = auto_match_column(cols, COLUMN_CANDIDATES["content"])
    col_trans = auto_match_column(cols, COLUMN_CANDIDATES["translation"])
    col_date = auto_match_column(cols, COLUMN_CANDIDATES["date"])
    col_id = auto_match_column(cols, COLUMN_CANDIDATES["id"])

    # å†…å®¹ä¼˜å…ˆçº§ï¼šç¿»è¯‘åˆ— > å†…å®¹åˆ—
    text_primary = col_trans or col_content

    return {
        "rating": col_rating,
        "title": col_title,         # å¯ç©º
        "text": text_primary,       # å¿…é¡»
        "content_raw": col_content, # å¯ç©ºï¼ˆç”¨äºæ’æŸ¥ï¼‰
        "translation": col_trans,   # å¯ç©º
        "date": col_date,           # å¯ç©º
        "id": col_id                # å¯ç©ºï¼ˆå¯è‡ªåŠ¨ç”Ÿæˆï¼‰
    }

# ======================================================
# 4) é¡µé¢
# ======================================================
st.title("ğŸ·ï¸ Amazon è¯„è®ºæ‰“æ ‡ç³»ç»Ÿï¼ˆè‡ªåŠ¨åˆ—æ˜ å°„ + å†…ç½®è¯„ä»·åº“ï¼‰")

tab1, tab2, tab3, tab4 = st.tabs([
    "1ï¸âƒ£ ä¸Šä¼ è¯„è®º & è‡ªåŠ¨æ˜ å°„",
    "2ï¸âƒ£ å†…ç½®è¯„ä»·åº“ï¼ˆå¯é€‰ç¼–è¾‘ï¼‰",
    "3ï¸âƒ£ ç”Ÿæˆ Promptï¼ˆ4æ˜Ÿä¼˜å…ˆå·®è¯„ï¼‰",
    "4ï¸âƒ£ å›å¡« & å¯¼å‡º"
])

# ======================================================
# Tab 1ï¼šä¸Šä¼ è¯„è®º & è‡ªåŠ¨æ˜ å°„ï¼ˆæ ¸å¿ƒæ”¹åŠ¨ï¼šä¸è®©ç”¨æˆ·ç‚¹ï¼‰
# ======================================================
with tab1:
    st.header("Step 1ï¼šä¸Šä¼ è¯„è®ºæ–‡ä»¶ â†’ ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«åˆ—ç»„åˆ â†’ ä¸€é”®é”å®š")

    uploaded = st.file_uploader("ä¸Šä¼ è¯„è®ºæ•°æ®ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx"])

    if uploaded:
        df_raw = load_file(uploaded)
        st.session_state.raw_df = df_raw

        st.success(f"âœ… è¯„è®ºæ•°æ®åŠ è½½æˆåŠŸï¼šåŸå§‹è¡Œæ•° = {len(df_raw)}")
        st.dataframe(df_raw.head(8))

        # è‡ªåŠ¨æ˜ å°„
        auto_map = auto_build_mapping(df_raw)
        st.session_state.col_map = auto_map

        st.markdown("### ğŸ” ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«åˆ°çš„åˆ—æ˜ å°„ï¼ˆé»˜è®¤ä¸éœ€è¦æ‰‹åŠ¨ç‚¹ï¼‰")
        st.json(auto_map)

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€ä½è¦æ±‚
        missing_critical = []
        if not auto_map["rating"]:
            missing_critical.append("ratingï¼ˆæ˜Ÿçº§åˆ—ï¼‰")
        if not auto_map["text"]:
            missing_critical.append("textï¼ˆå†…å®¹/ç¿»è¯‘åˆ—ï¼‰")

        if missing_critical:
            st.error("âŒ è‡ªåŠ¨è¯†åˆ«å¤±è´¥ï¼šç¼ºå°‘å…³é”®åˆ—ï¼š" + "ã€".join(missing_critical))
            st.info("è¯·åœ¨ä¸‹æ–¹ã€é«˜çº§è®¾ç½®ã€‘é‡Œæ‰‹åŠ¨æŒ‡å®šï¼ˆä»…åœ¨è¯†åˆ«å¤±è´¥æ—¶éœ€è¦ï¼‰ã€‚")
            with st.expander("é«˜çº§è®¾ç½®ï¼šæ‰‹åŠ¨ä¿®æ­£åˆ—æ˜ å°„ï¼ˆä»…è¯†åˆ«å¤±è´¥æ—¶ç”¨ï¼‰", expanded=True):
                cols = df_raw.columns.tolist()
                col_rating = st.selectbox("æ‰‹åŠ¨é€‰æ‹©æ˜Ÿçº§åˆ—", cols, index=0)
                col_text = st.selectbox("æ‰‹åŠ¨é€‰æ‹©å†…å®¹åˆ—ï¼ˆå»ºè®®é€‰ å†…å®¹(ç¿»è¯‘) ä¼˜å…ˆï¼‰", cols, index=0)
                col_title = st.selectbox("æ‰‹åŠ¨é€‰æ‹©æ ‡é¢˜åˆ—ï¼ˆå¯é€‰ï¼‰", ["--ä¸ä½¿ç”¨--"] + cols, index=0)
                col_date = st.selectbox("æ‰‹åŠ¨é€‰æ‹©æ—¶é—´åˆ—ï¼ˆå¯é€‰ï¼‰", ["--ä¸ä½¿ç”¨--"] + cols, index=0)
                col_id = st.selectbox("æ‰‹åŠ¨é€‰æ‹©IDåˆ—ï¼ˆå¯é€‰ï¼‰", ["--è‡ªåŠ¨ç”ŸæˆUUID--"] + cols, index=0)

                # è¦†ç›–auto_map
                auto_map["rating"] = col_rating
                auto_map["text"] = col_text
                auto_map["title"] = None if col_title == "--ä¸ä½¿ç”¨--" else col_title
                auto_map["date"] = None if col_date == "--ä¸ä½¿ç”¨--" else col_date
                auto_map["id"] = None if col_id == "--è‡ªåŠ¨ç”ŸæˆUUID--" else col_id
                st.session_state.col_map = auto_map
                st.warning("å·²ç”¨æ‰‹åŠ¨è®¾ç½®è¦†ç›–è‡ªåŠ¨è¯†åˆ«ã€‚è¯·ç»§ç»­é¢„è§ˆ/é”å®šã€‚")

        st.markdown("---")
        c1, c2, c3 = st.columns([1,1,2])
        with c1:
            preview = st.button("ğŸ” é¢„è§ˆæ¸…æ´—æ•ˆæœ", disabled=st.session_state.mapping_locked)
        with c2:
            lock = st.button("âœ… é”å®šæ˜ å°„å¹¶ç”Ÿæˆçœ‹æ¿", type="primary",
                             disabled=(st.session_state.mapping_locked))
        with c3:
            unlock = st.button("â™»ï¸ è§£é™¤é”å®š", disabled=(not st.session_state.mapping_locked))

        if unlock:
            st.session_state.mapping_locked = False
            st.session_state.main_df = None
            st.session_state.norm_df = None
            st.session_state.merged_full_df = None
            st.success("å·²è§£é™¤é”å®šï¼Œå¯é‡æ–°é¢„è§ˆ/é”å®šã€‚")

        def build_cleaned_frames(df_in: pd.DataFrame, m: dict):
            tmp = df_in.copy()

            # rating è§£æ
            tmp["rating_numeric"] = tmp[m["rating"]].apply(parse_rating)
            invalid_rating_cnt = int(tmp["rating_numeric"].isna().sum())

            valid = tmp.dropna(subset=["rating_numeric"]).copy()
            valid["rating_int"] = valid["rating_numeric"].round().astype(int)
            valid = valid[valid["rating_int"].between(1, 5)]

            # dateï¼ˆå¯é€‰ï¼‰
            time_parse_success = False
            if m.get("date"):
                valid["date_parsed"] = pd.to_datetime(valid[m["date"]], errors="coerce")
                time_parse_success = valid["date_parsed"].notna().sum() > 0

            # idï¼šä¼˜å…ˆç”¨è¯†åˆ«åˆ°çš„idåˆ—ï¼Œå¦åˆ™è‡ªåŠ¨uuid
            if m.get("id") and m["id"] in valid.columns:
                valid["sys_id"] = valid[m["id"]].astype(str)
                id_col = "sys_id"
            else:
                valid["sys_id"] = [str(uuid.uuid4())[:8] for _ in range(len(valid))]
                id_col = "sys_id"

            # textï¼štitleå¯é€‰æ‹¼æ¥
            title_col = m.get("title")
            text_col = m.get("text")
            if title_col and title_col in valid.columns:
                valid["__text__"] = (
                    valid[title_col].fillna("").astype(str).str.strip()
                    + " | "
                    + valid[text_col].fillna("").astype(str).str.strip()
                ).str.strip(" |")
            else:
                valid["__text__"] = valid[text_col].fillna("").astype(str)

            norm = valid[[id_col, "rating_int", "__text__"]].rename(columns={
                id_col: "id",
                "rating_int": "rating",
                "__text__": "text"
            }).copy()

            return valid, norm, invalid_rating_cnt, time_parse_success

        if preview:
            m = st.session_state.col_map
            valid, norm, invalid_cnt, time_ok = build_cleaned_frames(df_raw, m)

            # æŒ‡æ ‡
            raw_total = len(df_raw)
            valid_total = len(valid)
            neg_cnt = int((valid["rating_int"] <= 3).sum())
            neg_rate = (neg_cnt / valid_total * 100) if valid_total else 0
            severe_cnt = int((valid["rating_int"] <= 2).sum())
            severe_rate = (severe_cnt / valid_total * 100) if valid_total else 0

            st.subheader("ğŸ“Š é¢„è§ˆçœ‹æ¿ï¼ˆæœªé”å®šï¼‰")
            k1, k2, k3, k4, k5 = st.columns(5)
            k1.metric("åŸå§‹è¡Œæ•°", raw_total)
            k2.metric("æœ‰æ•ˆè¯„åˆ†è¡Œæ•°", valid_total)
            k3.metric("è¯„åˆ†è§£æå¤±è´¥", invalid_cnt)
            k4.metric("å·®è¯„å æ¯”(â‰¤3â­)", f"{neg_rate:.1f}%")
            k5.metric("ä¸¥é‡å·®è¯„(â‰¤2â­)", f"{severe_rate:.1f}%")

            st.markdown("### â­ æ˜Ÿçº§åˆ†å¸ƒï¼ˆ1â€“5ï¼‰")
            dist = valid["rating_int"].value_counts().reindex([1,2,3,4,5], fill_value=0).sort_index()
            st.bar_chart(dist)

            st.markdown("### ğŸ“ LLM è¾“å…¥æ–‡æœ¬é¢„è§ˆï¼ˆå‰ 5 æ¡ï¼‰")
            st.dataframe(norm.head(5))

            if time_ok:
                st.markdown("### ğŸ“ˆ è¯„è®ºæ—¶é—´è¶‹åŠ¿ï¼ˆæœˆåº¦ï¼‰")
                ts = valid.dropna(subset=["date_parsed"]).set_index("date_parsed").resample("M").size()
                st.line_chart(ts)
            else:
                st.info("æœªè¯†åˆ«åˆ°æ—¶é—´åˆ—æˆ–æ—¶é—´è§£æå¤±è´¥ï¼šå·²è·³è¿‡è¶‹åŠ¿åˆ†æã€‚")

        if lock:
            m = st.session_state.col_map
            valid, norm, invalid_cnt, _ = build_cleaned_frames(df_raw, m)
            st.session_state.main_df = valid
            st.session_state.norm_df = norm
            st.session_state.mapping_locked = True
            st.success("âœ… å·²é”å®šæ˜ å°„å¹¶ç”Ÿæˆæ ‡å‡†æ•°æ®ï¼Œå¯è¿›å…¥ Step 2/3ã€‚")

# ======================================================
# Tab 2ï¼šå†…ç½®è¯„ä»·åº“ï¼ˆå¯é€‰ç¼–è¾‘ï¼‰
# ======================================================
with tab2:
    st.header("Step 2ï¼šå†…ç½®è¯„ä»·åº“ï¼ˆé»˜è®¤å·²åŠ è½½ï¼Œä¸éœ€è¦ä¸Šä¼ ï¼‰")
    st.info("ä½ è¦ç”¨â€œæ–‡ä»¶è¯„ä»·åº“â€çš„æ­£å¼æ ‡ç­¾æ—¶ï¼ŒæŠŠå®ƒä»¬æ›¿æ¢åˆ°è¿™é‡Œï¼Œæˆ–è€…åœ¨ä¸‹æ–¹ç›´æ¥ç¼–è¾‘ã€‚")

    pos = st.session_state.tag_config["pos"]
    neg = st.session_state.tag_config["neg"]

    c1, c2 = st.columns(2)
    with c1:
        st.subheader("âœ… å¥½è¯„æ ‡ç­¾ï¼ˆPositiveï¼‰")
        pos_text = st.text_area("ä¸€è¡Œä¸€ä¸ªæ ‡ç­¾", value="\n".join(pos), height=260)
    with c2:
        st.subheader("âŒ å·®è¯„æ ‡ç­¾ï¼ˆNegativeï¼‰")
        neg_text = st.text_area("ä¸€è¡Œä¸€ä¸ªæ ‡ç­¾", value="\n".join(neg), height=260)

    if st.button("ä¿å­˜è¯„ä»·åº“ä¿®æ”¹"):
        pos_new = [x.strip() for x in pos_text.splitlines() if x.strip()]
        neg_new = [x.strip() for x in neg_text.splitlines() if x.strip()]
        st.session_state.tag_config = {
            "pos": pos_new,
            "neg": neg_new,
            "all": pos_new + neg_new
        }
        st.success(f"âœ… å·²ä¿å­˜ï¼šå¥½è¯„ {len(pos_new)} ä¸ªï¼Œå·®è¯„ {len(neg_new)} ä¸ª")

# ======================================================
# Tab 3ï¼šPrompt ç”Ÿæˆï¼ˆ4æ˜Ÿä¼˜å…ˆå·®è¯„ï¼‰
# ======================================================
with tab3:
    st.header("Step 3ï¼šç”Ÿæˆ Promptï¼ˆ4æ˜Ÿä¼˜å…ˆå·®è¯„ç‚¹ï¼‰")

    if not st.session_state.mapping_locked or st.session_state.norm_df is None:
        st.warning("è¯·å…ˆåœ¨ Step 1 é”å®šæ˜ å°„å¹¶ç”Ÿæˆæ ‡å‡†æ•°æ®ã€‚")
        st.stop()

    if not st.session_state.tag_config["pos"] or not st.session_state.tag_config["neg"]:
        st.warning("è¯„ä»·åº“ä¸ºç©ºï¼šè¯·åœ¨ Step 2 å…ˆé…ç½®å¥½è¯„/å·®è¯„æ ‡ç­¾ã€‚")
        st.stop()

    batch_size = st.number_input("æ¯æ‰¹æ¡æ•°", value=40, min_value=10, max_value=200, step=10)

    def build_prompt(chunk, mode, pos_tags, neg_tags):
        pos_str = ", ".join([f'"{t}"' for t in pos_tags])
        neg_str = ", ".join([f'"{t}"' for t in neg_tags])

        system = (
            "You are an expert customer review tagger.\n"
            "You MUST choose labels ONLY from the provided tag libraries.\n"
            "Return STRICT JSON only. No explanations. No extra text.\n"
            "Output schema: [{\"id\":\"...\",\"label\":\"\"}].\n"
            "If no suitable tag, label must be empty string \"\".\n"
        )

        if mode == "1-3":
            task = f"""
TASK:
These are 1-3 star reviews.
You MUST choose from NEGATIVE TAG LIBRARY only.

NEGATIVE TAG LIBRARY:
[{neg_str}]
"""
        elif mode == "5":
            task = f"""
TASK:
These are 5 star reviews.
You MUST choose from POSITIVE TAG LIBRARY only.

POSITIVE TAG LIBRARY:
[{pos_str}]
"""
        else:  # 4-star
            task = f"""
TASK:
These are 4 star reviews. PRIORITIZE complaints.
Rule:
1) If review contains ANY complaint/negative point, choose from NEGATIVE TAG LIBRARY.
2) Otherwise choose from POSITIVE TAG LIBRARY.
3) If still no suitable tag, output "".

NEGATIVE TAG LIBRARY:
[{neg_str}]

POSITIVE TAG LIBRARY:
[{pos_str}]
"""
        data = "DATA (JSON):\n" + json.dumps(chunk, ensure_ascii=False, indent=2)
        return f"{system}\n{task}\n{data}"

    if st.button("ç”Ÿæˆ Prompt"):
        df = st.session_state.norm_df
        pos_tags = st.session_state.tag_config["pos"]
        neg_tags = st.session_state.tag_config["neg"]

        groups = {
            "1-3": df[df["rating"] <= 3],
            "4": df[df["rating"] == 4],
            "5": df[df["rating"] == 5],
        }

        batches = []
        for mode, gdf in groups.items():
            if gdf.empty:
                continue
            records = gdf.to_dict("records")
            for i in range(0, len(records), int(batch_size)):
                chunk = records[i:i+int(batch_size)]
                batches.append({
                    "title": f"[{mode}æ˜Ÿ] æ‰¹æ¬¡ {i//int(batch_size)+1}ï¼ˆ{len(chunk)}æ¡ï¼‰",
                    "prompt": build_prompt(chunk, mode, pos_tags, neg_tags)
                })

        st.session_state.generated_batches = batches
        st.success(f"âœ… å·²ç”Ÿæˆ {len(batches)} ä¸ªä»»åŠ¡åŒ…")

    for b in st.session_state.generated_batches:
        with st.expander(b["title"]):
            st.text_area("Promptï¼ˆå¤åˆ¶ç»™æ¨¡å‹ï¼‰", b["prompt"], height=290)
            st.caption("åŠ¡å¿…è®©æ¨¡å‹åªè¾“å‡º JSONï¼ˆä¸å¸¦è§£é‡Šï¼‰ï¼Œå¦åˆ™å›å¡«ä¼šè§£æå¤±è´¥ã€‚")

# ======================================================
# Tab 4ï¼šå›å¡« & å¯¼å‡º
# ======================================================
with tab4:
    st.header("Step 4ï¼šç²˜è´´æ¨¡å‹ JSON â†’ å›å¡« â†’ å¯¼å‡º")

    if st.session_state.norm_df is None:
        st.warning("è¯·å…ˆå®Œæˆ Step 1 / Step 3ã€‚")
        st.stop()

    allowed_set = set(st.session_state.tag_config["all"])

    json_text = st.text_area("ç²˜è´´ LLM è¿”å› JSONï¼ˆå¯ä¸€æ¬¡ç²˜è´´å¤šæ‰¹æ¬¡ï¼‰", height=220)

    if st.button("åˆå¹¶ç»“æœ"):
        data = safe_parse_json(json_text)
        if not data or not isinstance(data, list):
            st.error("JSON è§£æå¤±è´¥ï¼šè¯·ç¡®ä¿è¿”å›æ ¼å¼ä¸º listï¼Œä¾‹å¦‚ï¼š[{'id':'xxx','label':'...'}]")
        else:
            res_df = pd.DataFrame(data)
            if "id" not in res_df.columns or "label" not in res_df.columns:
                st.error("è¿”å› JSON å¿…é¡»åŒ…å« id å’Œ label å­—æ®µã€‚")
            else:
                res_df["id"] = res_df["id"].astype(str)
                res_df["label"] = res_df["label"].apply(lambda x: validate_label(x, allowed_set))

                id_map = dict(zip(res_df["id"], res_df["label"]))

                df = st.session_state.norm_df.copy()
                if "AI_Label" not in df.columns:
                    df["AI_Label"] = ""

                df["AI_Label"] = df["id"].map(id_map).fillna(df["AI_Label"]).astype(str)
                st.session_state.norm_df = df

                st.success(f"âœ… åˆå¹¶å®Œæˆï¼šæœ¬æ¬¡åˆå¹¶ {len(res_df)} æ¡ï¼ˆåº“å¤–æ ‡ç­¾å·²è‡ªåŠ¨ç½®ç©ºï¼‰")
                st.dataframe(df.head(20))

                # åˆå¹¶å›ä¸»è¡¨ï¼ˆåŸå­—æ®µ+AI_Labelï¼‰
                if st.session_state.main_df is not None:
                    main = st.session_state.main_df.copy()
                    lab = df[["id", "AI_Label"]].copy()
                    # mainé‡Œsys_idå¯¹åº”norm_dfçš„id
                    if "sys_id" in main.columns:
                        main["sys_id"] = main["sys_id"].astype(str)
                        lab["id"] = lab["id"].astype(str)
                        merged = main.merge(lab, left_on="sys_id", right_on="id", how="left")
                        merged.drop(columns=["id"], inplace=True, errors="ignore")
                        st.session_state.merged_full_df = merged

    st.markdown("---")
    st.subheader("å¯¼å‡º")

    out1 = st.session_state.norm_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ä¸‹è½½ï¼šnormalizedï¼ˆid/rating/text/AI_Labelï¼‰", out1, "tagged_reviews_normalized.csv", "text/csv")

    if st.session_state.merged_full_df is not None:
        out2 = st.session_state.merged_full_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("â¬‡ï¸ ä¸‹è½½ï¼šfullï¼ˆåŸå§‹å­—æ®µ + AI_Labelï¼‰", out2, "tagged_reviews_full.csv", "text/csv")
