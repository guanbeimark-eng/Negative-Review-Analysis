import streamlit as st
import pandas as pd
import numpy as np
import json
import uuid
import re
from typing import List, Dict, Any, Optional

from openai import OpenAI

# =========================
# 0) App Config + (å¯é€‰)ç™»å½•
# =========================
st.set_page_config(page_title="è¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆä¸€é”®ç‰ˆï¼‰", page_icon="ğŸ·ï¸", layout="wide")

# å¦‚æœä½ ä¸éœ€è¦ç™»å½•ï¼Œç›´æ¥æŠŠè¿™æ®µåˆ æ‰å³å¯
ACCESS_PASSWORD = "admin123"
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def check_password():
    if st.session_state.get("password_input") == ACCESS_PASSWORD:
        st.session_state.logged_in = True
    else:
        st.error("å¯†ç é”™è¯¯")

if not st.session_state.logged_in:
    st.markdown("## ğŸ”’ ç³»ç»Ÿé”å®š")
    st.text_input("è®¿é—®å¯†ç ", type="password", key="password_input", on_change=check_password)
    st.stop()

# =========================
# 1) å†…ç½®è¯„ä»·åº“ï¼ˆé»˜è®¤ï¼‰
# ä½ åç»­æŠŠè¿™é‡Œæ›¿æ¢æˆä½ ä»¬â€œæ–‡ä»¶è¯„ä»·åº“â€çš„æ­£å¼æ ‡ç­¾å³å¯
# =========================
DEFAULT_TAG_LIBRARY = {
    "positive": [
        "ä½©æˆ´èˆ’é€‚", "æ”¯æ’‘æ€§å¥½", "ç¼“è§£å…³èŠ‚ä¸é€‚", "å°ºå¯¸åˆé€‚", "è´¨é‡å¥½",
        "æ€§ä»·æ¯”é«˜", "æ•ˆæœæ˜æ˜¾", "ç‰©æµ/å‘è´§å¿«", "å¤–è§‚å¥½çœ‹"
    ],
    "negative": [
        "å°ºç åå°", "å°ºç åå¤§", "å°ºç ä¸ä¸€è‡´", "ä¸é€‚åˆç”·å£«", "ç©¿æˆ´å›°éš¾",
        "è´¨é‡å·®", "ä¸æè¿°ä¸ç¬¦", "ä¸èˆ’é€‚/å‹’æ‰‹", "æ°”å‘³/å¼‚å‘³",
        "è€ç”¨æ€§å·®/æ˜“ç ´", "å‹åŠ›/å‹ç¼©æ„Ÿä¸è¶³"
    ]
}

# =========================
# 2) Session State
# =========================
defaults = {
    "raw_df": None,
    "main_df": None,      # æ¸…æ´—åä¸»è¡¨ï¼šåŸå­—æ®µ + rating_int + sys_id + __text__
    "norm_df": None,      # æ ‡å‡†è¡¨ï¼šid/rating/text/AI_Label
    "full_df": None,      # ä¸»è¡¨åˆå¹¶ AI_Label åçš„å¯¼å‡ºè¡¨
    "col_map": None,
    "tag_config": {
        "pos": DEFAULT_TAG_LIBRARY["positive"],
        "neg": DEFAULT_TAG_LIBRARY["negative"],
        "all": DEFAULT_TAG_LIBRARY["positive"] + DEFAULT_TAG_LIBRARY["negative"],
    },
}
for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v

# =========================
# 3) Utils
# =========================
def load_file(f) -> pd.DataFrame:
    name = f.name.lower()
    if name.endswith(".csv"):
        try:
            return pd.read_csv(f, encoding="utf-8")
        except UnicodeDecodeError:
            return pd.read_csv(f, encoding="gbk")
    # xlsx
    return pd.read_excel(f)

def parse_rating(x) -> float:
    """
    å…¼å®¹ rating: '4.0 out of 5 stars' / 'Rated 3' / '5' / 4.0
    """
    if pd.isna(x):
        return np.nan
    s = str(x)
    m = re.search(r"(\d+(?:\.\d+)?)", s)
    if not m:
        return np.nan
    try:
        return float(m.group(1))
    except Exception:
        return np.nan

COLUMN_CANDIDATES = {
    "rating": ["æ˜Ÿçº§", "rating", "Rating", "è¯„åˆ†", "Score"],
    "title": ["æ ‡é¢˜", "title", "Title", "headline", "summary"],
    "content": ["å†…å®¹", "content", "Content", "review", "Review", "è¯„è®ºå†…å®¹", "body", "text"],
    "translation": ["å†…å®¹(ç¿»è¯‘)", "ç¿»è¯‘", "translation", "Translated", "å†…å®¹ï¼ˆç¿»è¯‘ï¼‰"],
    "date": ["è¯„è®ºæ—¶é—´", "date", "Date", "review_date", "time", "æ—¶é—´", "è¯„è®ºæ—¥æœŸ"],
    "id": ["review_id", "id", "ID", "è¯„è®ºID", "uuid", "å”¯ä¸€ID"],
}

def auto_match_column(cols: List[str], candidates: List[str]) -> Optional[str]:
    # ç²¾ç¡®åŒ¹é…
    for c in candidates:
        if c in cols:
            return c
    # æ¨¡ç³ŠåŒ…å«åŒ¹é…
    for cand in candidates:
        cand_l = cand.lower()
        for col in cols:
            if cand_l in col.lower():
                return col
    return None

def auto_build_mapping(df: pd.DataFrame) -> Dict[str, Optional[str]]:
    cols = df.columns.tolist()
    col_rating = auto_match_column(cols, COLUMN_CANDIDATES["rating"])
    col_title = auto_match_column(cols, COLUMN_CANDIDATES["title"])
    col_content = auto_match_column(cols, COLUMN_CANDIDATES["content"])
    col_trans = auto_match_column(cols, COLUMN_CANDIDATES["translation"])
    col_date = auto_match_column(cols, COLUMN_CANDIDATES["date"])
    col_id = auto_match_column(cols, COLUMN_CANDIDATES["id"])

    # æ–‡æœ¬ä¼˜å…ˆç¿»è¯‘åˆ—
    text_primary = col_trans or col_content

    return {
        "rating": col_rating,
        "title": col_title,
        "text": text_primary,
        "date": col_date,
        "id": col_id,
        "content_raw": col_content,
        "translation": col_trans,
    }

def build_cleaned_frames(df_raw: pd.DataFrame, m: Dict[str, Optional[str]]):
    tmp = df_raw.copy()

    # rating è§£æ
    if not m.get("rating"):
        tmp["rating_numeric"] = np.nan
    else:
        tmp["rating_numeric"] = tmp[m["rating"]].apply(parse_rating)
    invalid_rating_cnt = int(tmp["rating_numeric"].isna().sum())

    valid = tmp.dropna(subset=["rating_numeric"]).copy()
    valid["rating_int"] = valid["rating_numeric"].round().astype(int)
    valid = valid[valid["rating_int"].between(1, 5)]

    # date è§£æï¼ˆå¯é€‰ï¼‰
    time_ok = False
    if m.get("date") and m["date"] in valid.columns:
        valid["date_parsed"] = pd.to_datetime(valid[m["date"]], errors="coerce")
        time_ok = valid["date_parsed"].notna().sum() > 0

    # sys_idï¼šä¼˜å…ˆç”¨æ–‡ä»¶è‡ªå¸¦ IDï¼Œå¦åˆ™ç”Ÿæˆ
    if m.get("id") and m["id"] in valid.columns:
        valid["sys_id"] = valid[m["id"]].astype(str)
    else:
        valid["sys_id"] = [str(uuid.uuid4())[:8] for _ in range(len(valid))]

    # textï¼štitle å¯é€‰æ‹¼æ¥
    if not m.get("text"):
        valid["__text__"] = ""
    else:
        if m.get("title") and m["title"] in valid.columns:
            valid["__text__"] = (
                valid[m["title"]].fillna("").astype(str).str.strip()
                + " | "
                + valid[m["text"]].fillna("").astype(str).str.strip()
            ).str.strip(" |")
        else:
            valid["__text__"] = valid[m["text"]].fillna("").astype(str)

    norm = valid[["sys_id", "rating_int", "__text__"]].rename(
        columns={"sys_id": "id", "rating_int": "rating", "__text__": "text"}
    ).copy()

    return valid, norm, invalid_rating_cnt, time_ok

def validate_label(label: str, allowed_set: set) -> str:
    lab = (label or "").strip()
    return lab if lab in allowed_set else ""

def strict_json_load(s: str) -> Optional[Any]:
    """
    å°½é‡ä»æ¨¡å‹è¾“å‡ºé‡ŒæŠ å‡º JSON listï¼ˆå³ä½¿å¤¹å¸¦äº†å…¶å®ƒå­—ï¼‰
    """
    if not s:
        return None
    s = s.strip().replace("```json", "").replace("```", "").strip()

    try:
        return json.loads(s)
    except Exception:
        pass

    # å°è¯•æå–ç¬¬ä¸€ä¸ª [...] æ®µ
    m = re.search(r"(\[\s*\{.*\}\s*\])", s, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            return None
    return None

# =========================
# 4) OpenAI è°ƒç”¨ï¼šä¸€é”®è‡ªåŠ¨æ‰“æ ‡
# =========================
def build_api_prompt(records: List[Dict[str, Any]],
                     mode: str,
                     pos_tags: List[str],
                     neg_tags: List[str]) -> str:
    pos_str = ", ".join([f'"{t}"' for t in pos_tags])
    neg_str = ", ".join([f'"{t}"' for t in neg_tags])

    header = (
        "ä½ æ˜¯ç”µå•†å®¢æˆ·è¯„è®ºçš„æ ‡ç­¾å½’ç±»ä¸“å®¶ã€‚\n"
        "ä½ å¿…é¡»ä¸¥æ ¼åªè¾“å‡º JSON listï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
        "[{\"id\":\"...\",\"label\":\"...\"}, ...]\n"
        "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡å­—ï¼Œä¸è¦è¾“å‡º markdown ä»£ç å—ã€‚\n"
        "label å¿…é¡»ä»ç»™å®šæ ‡ç­¾åº“ä¸­é€‰æ‹©ï¼›ä¸åŒ¹é…åˆ™è¾“å‡ºç©ºå­—ç¬¦ä¸² \"\"ã€‚\n"
    )

    if mode == "1-3":
        rules = f"\nè¿™äº›æ˜¯ 1-3 æ˜Ÿè¯„è®ºï¼šåªèƒ½ä»ã€å·®è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\nå·®è¯„æ ‡ç­¾åº“ï¼š[{neg_str}]\n"
    elif mode == "5":
        rules = f"\nè¿™äº›æ˜¯ 5 æ˜Ÿè¯„è®ºï¼šåªèƒ½ä»ã€å¥½è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\nå¥½è¯„æ ‡ç­¾åº“ï¼š[{pos_str}]\n"
    else:  # 4-star
        rules = (
            f"\nè¿™äº›æ˜¯ 4 æ˜Ÿè¯„è®ºï¼šä¼˜å…ˆæ‰¾å·®è¯„ç‚¹ã€‚\n"
            "è§„åˆ™ï¼š\n"
            "1) åªè¦æœ‰ä»»ä½•æŠ±æ€¨/ä¸æ»¡æ„/ç¼ºç‚¹ï¼Œå°±ä¼˜å…ˆä»ã€å·®è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\n"
            "2) å¦‚æœå®Œå…¨æ˜¯å¤¸èµï¼Œå†ä»ã€å¥½è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\n"
            "3) ä¸åŒ¹é…è¾“å‡ºç©ºå­—ç¬¦ä¸²ã€‚\n"
            f"å·®è¯„æ ‡ç­¾åº“ï¼š[{neg_str}]\n"
            f"å¥½è¯„æ ‡ç­¾åº“ï¼š[{pos_str}]\n"
        )

    payload = "æ•°æ®å¦‚ä¸‹ï¼ˆJSONï¼‰ï¼š\n" + json.dumps(records, ensure_ascii=False)
    return header + rules + "\n" + payload

def call_openai_tagging(client: OpenAI,
                        model: str,
                        prompt: str,
                        max_retries: int = 2) -> List[Dict[str, str]]:
    """
    å¿…é¡»è¿”å›ï¼š[{id,label}, ...]
    å¤±è´¥ä¼šé‡è¯•ï¼ˆåŠ å¼ºçº¦æŸï¼‰
    """
    last_text = ""
    for attempt in range(max_retries + 1):
        resp = client.responses.create(
            model=model,
            input=prompt
        )
        text = getattr(resp, "output_text", "") or ""
        last_text = text

        obj = strict_json_load(text)
        if isinstance(obj, list) and all(isinstance(x, dict) and "id" in x and "label" in x for x in obj):
            return [{"id": str(x["id"]), "label": str(x.get("label", "")).strip()} for x in obj]

        prompt = (
            "å†æ¬¡å¼ºè°ƒï¼šä½ åªèƒ½è¾“å‡º JSON listï¼Œä¸”æ¯ä¸ªå…ƒç´ åªå…è®¸åŒ…å« id å’Œ label ä¸¤ä¸ªé”®ã€‚\n"
            "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šï¼Œä¸è¦è¾“å‡º ```ã€‚\n\n"
            + prompt
        )

    raise ValueError(f"æ¨¡å‹è¾“å‡ºæ— æ³•è§£æä¸º JSON list[{ '{id,label}' }]ï¼ŒåŸå§‹è¾“å‡ºç‰‡æ®µï¼š{last_text[:500]}")

# =========================
# 5) UIï¼šçœŸæ­£å‚»ç“œå¼ï¼ˆä¸Šä¼  â†’ ä¸€é”®æ‰“æ ‡ â†’ ä¸‹è½½ï¼‰
# =========================
st.title("ğŸ·ï¸ è¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆä¸Šä¼  â†’ ä¸€é”®æ‰“æ ‡ â†’ å¯¼å‡ºï¼‰")
st.caption("ç”¨æˆ·æ— éœ€å¤åˆ¶/ç²˜è´´ä»»ä½• JSONã€‚")

# API Keyï¼šä» Streamlit Secrets è¯»å–ï¼ˆæœ€æ¨èï¼‰
api_key = st.secrets.get("OPENAI_API_KEY", "")
if not api_key:
    st.warning("æœªæ£€æµ‹åˆ° OPENAI_API_KEYã€‚è¯·åœ¨ Streamlit Cloud çš„ Settings â†’ Secrets ä¸­é…ç½®ã€‚")

model_name = st.text_input("æ¨¡å‹åï¼ˆé»˜è®¤ gpt-5.2ï¼‰", value="gpt-5.2")

uploaded = st.file_uploader("ä¸Šä¼ è¯„è®ºæ–‡ä»¶ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx"])

with st.expander("è¯„ä»·åº“ï¼ˆå¯é€‰ç¼–è¾‘ï¼šé»˜è®¤å·²å†…ç½®ï¼‰", expanded=False):
    c1, c2 = st.columns(2)
    with c1:
        pos_text = st.text_area("å¥½è¯„æ ‡ç­¾ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰", value="\n".join(st.session_state.tag_config["pos"]), height=220)
    with c2:
        neg_text = st.text_area("å·®è¯„æ ‡ç­¾ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰", value="\n".join(st.session_state.tag_config["neg"]), height=220)

    if st.button("ä¿å­˜è¯„ä»·åº“"):
        pos = [x.strip() for x in pos_text.splitlines() if x.strip()]
        neg = [x.strip() for x in neg_text.splitlines() if x.strip()]
        st.session_state.tag_config = {"pos": pos, "neg": neg, "all": pos + neg}
        st.success(f"å·²ä¿å­˜ï¼šå¥½è¯„ {len(pos)} ä¸ª / å·®è¯„ {len(neg)} ä¸ª")

if uploaded:
    # 1) è¯»å– + è‡ªåŠ¨æ˜ å°„ + æ¸…æ´—
    df_raw = load_file(uploaded)
    st.session_state.raw_df = df_raw

    m = auto_build_mapping(df_raw)
    st.session_state.col_map = m

    if not m.get("rating") or not m.get("text"):
        st.error("âŒ è‡ªåŠ¨è¯†åˆ«å¤±è´¥ï¼šç¼ºå°‘æ˜Ÿçº§åˆ—æˆ–å†…å®¹åˆ—ã€‚å»ºè®®åˆ—åä½¿ç”¨ï¼šæ˜Ÿçº§ / å†…å®¹ / å†…å®¹(ç¿»è¯‘)")
        st.json(m)
        st.stop()

    valid, norm, invalid_cnt, time_ok = build_cleaned_frames(df_raw, m)
    st.session_state.main_df = valid
    st.session_state.norm_df = norm.copy()
    st.session_state.full_df = None

    # 2) çœ‹æ¿ï¼ˆè‡ªåŠ¨ï¼‰
    raw_total = len(df_raw)
    valid_total = len(valid)
    neg_cnt = int((valid["rating_int"] <= 3).sum())
    neg_rate = (neg_cnt / valid_total * 100) if valid_total else 0
    severe_cnt = int((valid["rating_int"] <= 2).sum())
    severe_rate = (severe_cnt / valid_total * 100) if valid_total else 0

    st.subheader("ğŸ“Š è‡ªåŠ¨çœ‹æ¿")
    k1, k2, k3, k4, k5 = st.columns(5)
    k1.metric("åŸå§‹è¡Œæ•°", raw_total)
    k2.metric("æœ‰æ•ˆè¯„åˆ†è¡Œæ•°", valid_total)
    k3.metric("è¯„åˆ†è§£æå¤±è´¥", invalid_cnt)
    k4.metric("å·®è¯„å æ¯”(â‰¤3â­)", f"{neg_rate:.1f}%")
    k5.metric("ä¸¥é‡å·®è¯„(â‰¤2â­)", f"{severe_rate:.1f}%")

    dist = valid["rating_int"].value_counts().reindex([1,2,3,4,5], fill_value=0).sort_index()
    st.bar_chart(dist)

    st.subheader("ğŸ” æ•°æ®é¢„è§ˆï¼ˆå‰ 8 æ¡ï¼‰")
    st.dataframe(norm.head(8))

    with st.expander("æŸ¥çœ‹ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«çš„åˆ—æ˜ å°„"):
        st.json(m)

    st.markdown("---")
    st.subheader("ğŸš€ ä¸€é”®è‡ªåŠ¨æ‰“æ ‡ï¼ˆä¸éœ€è¦å¤åˆ¶ç²˜è´´ï¼‰")

    batch_size = st.slider("æ¯æ‰¹æ¡æ•°ï¼ˆè¶Šå¤§è¶Šå¿«ï¼Œä½†æ›´åƒä¸Šä¸‹æ–‡ï¼‰", 20, 120, 60, 10)

    if st.button("ä¸€é”®è‡ªåŠ¨æ‰“æ ‡å¹¶ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶", type="primary"):
        if not api_key:
            st.error("æœªé…ç½® OPENAI_API_KEYã€‚è¯·åˆ° Streamlit Cloud â†’ Settings â†’ Secrets è®¾ç½®åå†è¯•ã€‚")
            st.stop()

        client = OpenAI(api_key=api_key)

        df = st.session_state.norm_df.copy()
        pos_tags = st.session_state.tag_config["pos"]
        neg_tags = st.session_state.tag_config["neg"]
        allowed_set = set(st.session_state.tag_config["all"])

        if "AI_Label" not in df.columns:
            df["AI_Label"] = ""

        groups = {
            "1-3": df[df["rating"] <= 3],
            "4": df[df["rating"] == 4],
            "5": df[df["rating"] == 5],
        }

        total_jobs = 0
        for g in groups.values():
            if len(g):
                total_jobs += int(np.ceil(len(g) / int(batch_size)))

        progress = st.progress(0.0)
        done = 0

        for mode, gdf in groups.items():
            if gdf.empty:
                continue

            records = gdf.to_dict("records")
            for i in range(0, len(records), int(batch_size)):
                chunk = records[i:i+int(batch_size)]
                prompt = build_api_prompt(chunk, mode, pos_tags, neg_tags)

                try:
                    results = call_openai_tagging(client, model_name, prompt, max_retries=2)
                except Exception as e:
                    st.error(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆ{mode}æ˜Ÿ æ‰¹æ¬¡ {i//int(batch_size)+1}ï¼‰ï¼š{e}")
                    st.stop()

                id_map = {r["id"]: validate_label(r["label"], allowed_set) for r in results}
                mask = df["id"].isin(id_map.keys())
                df.loc[mask, "AI_Label"] = df.loc[mask, "id"].map(id_map).fillna(df.loc[mask, "AI_Label"]).astype(str)

                done += 1
                progress.progress(min(1.0, done / max(1, total_jobs)))

        st.session_state.norm_df = df

        main = st.session_state.main_df.copy()
        lab = df[["id", "AI_Label"]].copy()
        main["sys_id"] = main["sys_id"].astype(str)
        lab["id"] = lab["id"].astype(str)
        merged = main.merge(lab, left_on="sys_id", right_on="id", how="left")
        merged.drop(columns=["id"], inplace=True, errors="ignore")
        st.session_state.full_df = merged

        st.success("âœ… è‡ªåŠ¨æ‰“æ ‡å®Œæˆï¼å¯ç›´æ¥ä¸‹è½½å¯¼å‡ºæ–‡ä»¶ã€‚")
        st.dataframe(df.head(20))

st.markdown("---")
st.subheader("â¬‡ï¸ å¯¼å‡º")

if st.session_state.norm_df is not None:
    out_norm = st.session_state.norm_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½ï¼šnormalizedï¼ˆid/rating/text/AI_Labelï¼‰",
        out_norm,
        "tagged_reviews_normalized.csv",
        "text/csv"
    )

if st.session_state.full_df is not None:
    out_full = st.session_state.full_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½ï¼šfullï¼ˆåŸå§‹å­—æ®µ + AI_Labelï¼‰",
        out_full,
        "tagged_reviews_full.csv",
        "text/csv"
    )
