import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sentence_transformers import SentenceTransformer, util
import torch
import re
import io

# =========================
# 0. é¡µé¢é…ç½®ä¸å®‰å…¨éªŒè¯
# =========================
st.set_page_config(
    page_title="AI è¯„è®ºç²¾ç»†åŒ–åˆ†æç³»ç»Ÿ (NLP Engineer Ver.)",
    page_icon="ğŸ”¬",
    layout="wide"
)

# è§£å†³ Matplotlib ä¸­æ–‡ä¹±ç é—®é¢˜ (å°è¯•ä½¿ç”¨ç³»ç»Ÿé€šç”¨å­—ä½“)
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

ACCESS_PASSWORD = "admin123" 

if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def check_password():
    if st.session_state["password_input"] == ACCESS_PASSWORD:
        st.session_state.logged_in = True
    else:
        st.error("å¯†ç é”™è¯¯")

if not st.session_state.logged_in:
    st.markdown("## ğŸ”’ ç³»ç»Ÿé”å®š")
    st.text_input("è¯·è¾“å…¥è®¿é—®å¯†ç ", type="password", key="password_input", on_change=check_password)
    st.stop() 

# =========================
# 1. æ ‡ç­¾åº“å®šä¹‰ (ä¸¥æ ¼éµå®ˆ)
# =========================

# å¥½è¯„æ ‡ç­¾åº“ (å›ºå®šé›†åˆ)
POS_LABELS_LIST = [
    "é¢æ–™èˆ’é€‚", "è´¨é‡å¾ˆå¥½", "æœ‰åŠ©äºé”»ç‚¼", "æœ‰åŠ©äºç¼“è§£ç–¼ç—›", "ä¿æš–", "èˆ’é€‚è´´åˆ", 
    "æœ‰å‹ç¼©æ„Ÿ", "æŠ“æ¡å¼æœ‰æ•ˆ", "åˆèº«", "æœ‰åŠ©äºå…³èŠ‚ç‚/æ‰³æœºæŒ‡", "å¢åŠ æ‰‹æŒ‡çµæ´»", 
    "ä¿ƒè¿›è¡€æ¶²å¾ªç¯", "è€ç”¨", "ç¼“è§£ä¸é€‚", "è½»ç›ˆ", "è¦†ç›–æ•´ä¸ªæ‰‹æŒ‡", "æœ‰åŠ©äºé˜²æ­¢å—ä¼¤"
]

# å·®è¯„æ ‡ç­¾åº“ (æ²¿ç”¨æ—§ç‰ˆé€»è¾‘ï¼Œè¡¥å……å®Œæ•´ä»¥è¦†ç›–å¸¸è§å·®è¯„)
NEG_LABELS_LIST = [
    "æ— æ•ˆ/æ²¡æœ‰ä½œç”¨", "ç¼çº¿å¼€è£‚/ç ´æŸ", "æ”¶åˆ°äºŒæ‰‹/è„æ±¡", "é¢æ–™è´¨é‡å·®/å»‰ä»·", 
    "å°ºç å¤ªå°/å¤ªç´§", "å°ºç å¤ªå¤§/å¤ªæ¾", "æ¥ç¼å¤„ç£¨æ‰‹/ä¸é€‚", "ä¸è€ç”¨/ä¸€æ¬¡æ€§", 
    "è¿‡æ•/çš®ç–¹/å‘ç—’", "å¤ªæ»‘/æ²¡æœ‰æŠ“æ¡åŠ›", "æ•°é‡ä¸ç¬¦/å‘é”™è´§", "å¯¼è‡´è¡€æ¶²å¾ªç¯å—é˜»"
]

# =========================
# 2. AI æ¨¡å‹åŠ è½½
# =========================
@st.cache_resource
def load_model():
    # ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹å¤„ç†ä¸­è‹±æ–‡è¯­ä¹‰
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# =========================
# 3. æ ¸å¿ƒ NLP å¼•æ“ï¼šæ‹†å¥ä¸åŒ¹é…
# =========================

def split_into_sentences(text):
    """
    è¯­ä¹‰æ‹†è§£ï¼šå°†é•¿è¯„è®ºæ‹†åˆ†ä¸ºç‹¬ç«‹å¥å­/è¯­ä¹‰å•å…ƒã€‚
    æ”¯æŒä¸­è‹±æ–‡æ ‡ç‚¹åŠæ¢è¡Œç¬¦ã€‚
    """
    if not isinstance(text, str):
        return []
    # ä½¿ç”¨æ­£åˆ™æŒ‰ . ! ? ; ã€‚ ï¼ï¼Ÿ ï¼›ä»¥åŠæ¢è¡Œç¬¦è¿›è¡Œåˆ‡åˆ†
    sentences = re.split(r'[.!?;ã€‚ï¼ï¼Ÿï¼›\n]+', text)
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    return [s.strip() for s in sentences if s.strip()]

def analyze_single_review(row_idx, rating, full_text, model, threshold=0.40):
    """
    å¯¹å•æ¡è¯„è®ºè¿›è¡Œç»†ç²’åº¦åˆ†æï¼Œè¿”å›å¤šä¸ªç»“æ„åŒ–ç»“æœã€‚
    """
    sentences = split_into_sentences(full_text)
    analyzed_results = []
    
    # é¢„ç¼–ç æ ‡ç­¾åº“ (Tensor)
    pos_embeddings = model.encode(POS_LABELS_LIST, convert_to_tensor=True)
    neg_embeddings = model.encode(NEG_LABELS_LIST, convert_to_tensor=True)

    # è¯„è®ºæ•´ä½“æƒ…æ„ŸåŸºè°ƒ (ç®€å•è§„åˆ™ï¼š<=3æ˜Ÿä¸ºè´Ÿå‘ï¼Œ>=4æ˜Ÿä¸ºæ­£å‘)
    review_polarity_base = "negative" if rating <= 3 else "positive"

    if not sentences:
        # å¦‚æœè¯„è®ºä¸ºç©ºæˆ–æ— æ³•æ‹†åˆ†ï¼Œç›´æ¥è¿”å›æ•´å¥çš„å…œåº•
        return [{
            "review_id": row_idx,
            "original_review": full_text,
            "sentence": str(full_text),
            "polarity": review_polarity_base,
            "label": "å·®è¯„å…¶ä»–" if review_polarity_base == "negative" else "å¥½è¯„å…¶ä»–",
            "evidence": str(full_text),
            "confidence": 0.5
        }]

    for sent in sentences:
        # å¿½ç•¥å¤ªçŸ­çš„æ— æ„ä¹‰ç‰‡æ®µ (å¦‚ "OK", "å—¯")
        if len(sent) < 2:
            continue

        # ç¼–ç å½“å‰å¥å­
        sent_embedding = model.encode(sent, convert_to_tensor=True)

        # è®¡ç®—ç›¸ä¼¼åº¦
        pos_scores = util.cos_sim(sent_embedding, pos_embeddings)[0]
        neg_scores = util.cos_sim(sent_embedding, neg_embeddings)[0]

        best_pos_score = torch.max(pos_scores).item()
        best_pos_idx = torch.argmax(pos_scores).item()
        
        best_neg_score = torch.max(neg_scores).item()
        best_neg_idx = torch.argmax(neg_scores).item()

        # å†³ç­–é€»è¾‘ (Winner Takes All for this sentence)
        matched_label = None
        matched_polarity = None
        confidence = 0.0

        # 1. æ¯”è¾ƒæ­£å‘å’Œè´Ÿå‘çš„æœ€é«˜åˆ†
        if best_pos_score > best_neg_score:
            # å€¾å‘äºå¥½è¯„
            if best_pos_score > threshold:
                matched_label = POS_LABELS_LIST[best_pos_idx]
                matched_polarity = "positive"
                confidence = best_pos_score
            else:
                # æ²¡è¿‡é˜ˆå€¼ï¼Œä½†å¥å­çœ‹èµ·æ¥æ˜¯ä¸­æ€§/æ­£å‘çš„
                # è¿™é‡Œæˆ‘ä»¬åˆ©ç”¨æ•´æ¡è¯„è®ºçš„æ˜Ÿçº§åšå…œåº•
                if review_polarity_base == "positive":
                    matched_label = "å¥½è¯„å…¶ä»–"
                    matched_polarity = "positive"
                    confidence = 0.3 # ä½ç½®ä¿¡åº¦
                else:
                    # æ˜Ÿçº§æ˜¯å·®è¯„ï¼Œä½†è¿™å¥è¯æ²¡åŒ¹é…åˆ°å·®è¯„åº“ï¼Œå¯èƒ½æ˜¯ä¸€å¥åºŸè¯æˆ–â€œå…¶ä»–â€
                    # æš‚æ—¶å¿½ç•¥ï¼Œé™¤éå®ƒæ˜¯è¯¥è¯„è®ºå”¯ä¸€çš„å¥å­
                    pass 
        else:
            # å€¾å‘äºå·®è¯„
            if best_neg_score > threshold:
                matched_label = NEG_LABELS_LIST[best_neg_idx]
                matched_polarity = "negative"
                confidence = best_neg_score
            else:
                if review_polarity_base == "negative":
                    matched_label = "å·®è¯„å…¶ä»–"
                    matched_polarity = "negative"
                    confidence = 0.3
                else:
                    pass

        # å¦‚æœå¥å­æ²¡åŒ¹é…åˆ°ä»»ä½•å…·ä½“æ ‡ç­¾ï¼Œä¸”è¢«åˆ¤å®šä¸ºâ€œå…¶ä»–â€ï¼Œå­˜å…¥ç»“æœ
        if matched_label:
            analyzed_results.append({
                "review_id": row_idx,
                "original_review": full_text,
                "sentence": sent,
                "polarity": matched_polarity,
                "label": matched_label,
                "evidence": sent, # å¼ºè¯æ®ï¼šç›´æ¥å¼•ç”¨åŸå¥
                "confidence": round(confidence, 4)
            })

    # å…œåº•é€»è¾‘ï¼šå¦‚æœæ•´æ¡è¯„è®ºæ‹†å®Œåï¼Œè¿ä¸€ä¸ªæ ‡ç­¾éƒ½æ²¡æ‰“ä¸Šï¼ˆæ‰€æœ‰å¥å­éƒ½ä½äºé˜ˆå€¼ä¸”è¢«å¿½ç•¥ï¼‰
    if not analyzed_results:
        fallback_label = "å·®è¯„å…¶ä»–" if review_polarity_base == "negative" else "å¥½è¯„å…¶ä»–"
        analyzed_results.append({
            "review_id": row_idx,
            "original_review": full_text,
            "sentence": "(æ•´æ®µè¯­ä¹‰æ¨¡ç³Š)",
            "polarity": review_polarity_base,
            "label": fallback_label,
            "evidence": full_text,
            "confidence": 0.0
        })

    return analyzed_results

# =========================
# 4. è¾…åŠ©å·¥å…·
# =========================
def load_file(f):
    if f.name.lower().endswith(".csv"):
        try: return pd.read_csv(f, encoding="utf-8")
        except: return pd.read_csv(f, encoding="gbk")
    return pd.read_excel(f)

def parse_rating_strict(x):
    """å¼ºåˆ¶æå–è¯„åˆ†æ•´æ•°"""
    if pd.isna(x): return np.nan
    s = str(x)
    m = re.search(r"(\d+(\.\d+)?)", s)
    if m:
        val = float(m.group(1))
        val_int = int(round(val))
        return max(1, min(5, val_int))
    return np.nan

# =========================
# 5. ä¸»ç¨‹åº UI
# =========================
st.title("ğŸ”¬ AI è¯„è®ºç²¾ç»†åŒ–åˆ†æç³»ç»Ÿ")
st.markdown("""
**æ ¸å¿ƒé€»è¾‘æ›´æ–°ï¼š**
1. **è¯­ä¹‰æ‹†è§£**ï¼šè‡ªåŠ¨å°†é•¿è¯„è®ºæ‹†åˆ†ä¸ºç‹¬ç«‹å¥å­ï¼Œåˆ†åˆ«æ‰“æ ‡ï¼ˆè§£å†³ä¸€æ¡è¯„è®ºæ—¢å¥½åˆåçš„é—®é¢˜ï¼‰ã€‚
2. **å¼ºè¯æ®çº¦æŸ**ï¼šæ ‡ç­¾å¿…é¡»å¯¹åº”åŸæ–‡çš„å…·ä½“å¥å­ (`evidence`)ã€‚
3. **å…œåº•è§„åˆ™**ï¼šæœªåŒ¹é…åˆ°åº“çš„è¯­ä¹‰ï¼Œä¾æ®æ˜Ÿçº§å½’å…¥â€œå¥½è¯„å…¶ä»–â€æˆ–â€œå·®è¯„å…¶ä»–â€ã€‚
""")

with st.spinner("æ­£åœ¨åŠ è½½ NLP è¯­ä¹‰æ¨¡å‹..."):
    model = load_model()

uploaded = st.file_uploader("ä¸Šä¼ è¯„è®ºæ–‡ä»¶ (CSV/Excel)", type=["csv", "xlsx"])

if uploaded:
    with st.spinner('æ­£åœ¨é€å¥æ‹†è§£å¹¶åˆ†æè¯­ä¹‰...'):
        df_raw = load_file(uploaded)
        
        # 1. å­—æ®µè¯†åˆ«
        all_cols = df_raw.columns.tolist()
        rating_col = next((c for c in all_cols if "æ˜Ÿ" in str(c) or "rating" in str(c).lower()), all_cols[0])
        text_col = next((c for c in all_cols if "å†…å®¹" in str(c) or "review" in str(c).lower() or "text" in str(c).lower()), all_cols[1])
        
        # 2. æ¸…æ´—
        df_raw["rating_clean"] = df_raw[rating_col].apply(parse_rating_strict)
        df_raw = df_raw.dropna(subset=["rating_clean"])
        df_raw["text_clean"] = df_raw[text_col].astype(str).fillna("")
        
        # 3. æ ¸å¿ƒè¿ç®—ï¼šç”Ÿæˆç»“æ„åŒ–æ‰“æ ‡è¡¨ (Granular DataFrame)
        all_structured_data = []
        
        # è¿›åº¦æ¡
        progress_bar = st.progress(0)
        total_rows = len(df_raw)
        
        for idx, row in df_raw.iterrows():
            if idx % 10 == 0: progress_bar.progress(idx / total_rows)
            
            # è°ƒç”¨æ‹†å¥åˆ†æå‡½æ•°
            results = analyze_single_review(
                row_idx=idx, # ä½¿ç”¨ç´¢å¼•ä½œä¸º ID
                rating=row["rating_clean"],
                full_text=row["text_clean"],
                model=model
            )
            all_structured_data.extend(results)
            
        progress_bar.empty()
        
        # ç”Ÿæˆæœ€ç»ˆ DataFrame
        detailed_df = pd.DataFrame(all_structured_data)
        
    st.success(f"âœ… åˆ†æå®Œæˆï¼åŸæ•°æ® {len(df_raw)} æ¡ï¼Œæ‹†è§£å‡º {len(detailed_df)} ä¸ªè¯­ä¹‰å•å…ƒã€‚")

    # =========================
    # A: ç»“æ„åŒ–æ•°æ®å±•ç¤º
    # =========================
    st.markdown("---")
    st.header("1. ç»“æ„åŒ–æ‰“æ ‡ç»“æœ (Structured Data)")
    st.markdown("æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªâ€œè¯­ä¹‰å•å…ƒâ€ï¼Œè€Œéä¸€æ¡å®Œæ•´çš„è¯„è®ºã€‚")
    
    st.dataframe(
        detailed_df[["review_id", "label", "evidence", "sentence", "confidence"]], 
        use_container_width=True,
        height=400
    )

    # =========================
    # B: ç»Ÿè®¡å¯è§†åŒ– (Matplotlib é™çº§æ–¹æ¡ˆ)
    # =========================
    st.markdown("---")
    st.header("2. æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Top 10 æ ‡ç­¾åˆ†å¸ƒ")
        # ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
        label_counts = detailed_df["label"].value_counts().head(10)
        
        # ä½¿ç”¨ Matplotlib ç»˜å›¾
        fig, ax = plt.subplots(figsize=(8, 5))
        # é¢œè‰²æ˜ å°„ï¼šå¥½è¯„ç»¿ï¼Œå·®è¯„çº¢ï¼Œå…¶ä»–ç°
        colors = []
        for lbl in label_counts.index:
            if "å…¶ä»–" in lbl: colors.append("#95a5a6")
            elif lbl in POS_LABELS_LIST: colors.append("#2ecc71")
            else: colors.append("#e74c3c")
            
        bars = ax.barh(label_counts.index, label_counts.values, color=colors)
        ax.invert_yaxis() # ç¿»è½¬Yè½´è®©ç¬¬ä¸€ååœ¨ä¸Šé¢
        ax.set_xlabel("Mentions")
        ax.set_title("Label Frequency")
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for bar in bars:
            width = bar.get_width()
            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, 
                    f'{int(width)}', ha='left', va='center')
            
        st.pyplot(fig)

    with col2:
        st.subheader("æƒ…æ„Ÿå æ¯” (æ‹†å¥å)")
        polarity_counts = detailed_df["polarity"].value_counts()
        
        fig2, ax2 = plt.subplots(figsize=(6, 6))
        ax2.pie(
            polarity_counts.values, 
            labels=polarity_counts.index, 
            autopct='%1.1f%%', 
            colors=["#e74c3c", "#2ecc71", "#3498db"],
            startangle=90
        )
        ax2.set_title("Polarity Distribution (Sentence Level)")
        st.pyplot(fig2)

    # =========================
    # C: è¯æ®å›æº¯å·¥å…·
    # =========================
    st.markdown("---")
    st.header("3. è¯æ®å›æº¯ (Traceability)")
    
    selected_label = st.selectbox("é€‰æ‹©ä¸€ä¸ªæ ‡ç­¾æŸ¥çœ‹è¯æ®:", detailed_df["label"].unique())
    
    evidence_df = detailed_df[detailed_df["label"] == selected_label][["review_id", "evidence", "original_review"]]
    
    if not evidence_df.empty:
        st.write(f"å…±æ‰¾åˆ° {len(evidence_df)} æ¡è¯æ®ï¼š")
        for i, row in evidence_df.head(5).iterrows():
            with st.expander(f"Review #{row['review_id']}: \"{row['evidence']}\""):
                st.info(f"**å®Œæ•´åŸæ–‡:** {row['original_review']}")
    else:
        st.write("æ— æ•°æ®")

    # =========================
    # ä¸‹è½½åŒº
    # =========================
    st.markdown("---")
    
    # å¯¼å‡º CSV
    csv_buffer = detailed_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="â¬‡ï¸ ä¸‹è½½ç»“æ„åŒ–æ‰“æ ‡ç»“æœ (CSV)",
        data=csv_buffer,
        file_name="structured_analysis_result.csv",
        mime="text/csv"
    )
    
    # å¯¼å‡º Excel
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        detailed_df.to_excel(writer, index=False, sheet_name='Structured_Data')
        # åŒæ—¶ä¹ŸæŠŠåŸå§‹æ•°æ®æ”¾è¿›å»æ–¹ä¾¿å¯¹æ¯”
        df_raw.to_excel(writer, index=False, sheet_name='Raw_Data')
        
    st.download_button(
        label="â¬‡ï¸ ä¸‹è½½å®Œæ•´åˆ†ææŠ¥è¡¨ (Excel)",
        data=buffer.getvalue(),
        file_name="structured_analysis_report.xlsx",
        mime="application/vnd.ms-excel"
    )
