import streamlit as st
import pandas as pd
import numpy as np
import re
import math
from collections import Counter, defaultdict

st.set_page_config(page_title="äºšé©¬é€Šè¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆæƒé‡å­¦ä¹ ç‰ˆ/æ— æ¨¡å‹ï¼‰", page_icon="ğŸ·ï¸", layout="wide")

# =========================
# 1) ä½ çš„æ­£å¼æ ‡ç­¾åº“ï¼ˆåªè¾“å‡ºè¿™äº›æ ‡ç­¾ï¼‰
# =========================
POS_LABELS = [
    "é¢æ–™èˆ’é€‚","è´¨é‡å¾ˆå¥½","æœ‰åŠ©äºé”»ç‚¼","æœ‰åŠ©äºç¼“è§£ç–¼ç—›","ä¿æš–","èˆ’é€‚è´´åˆ","æœ‰å‹ç¼©æ„Ÿ","æŠ“æ¡å¼æœ‰æ•ˆ","åˆèº«",
    "æœ‰åŠ©äºå…³èŠ‚ç‚/æ‰³æœºæŒ‡","å¢åŠ æ‰‹æŒ‡çµæ´»","ä¿ƒè¿›è¡€æ¶²å¾ªç¯","è€ç”¨","ç¼“è§£ä¸é€‚","è½»ç›ˆ","è¦†ç›–æ•´ä¸ªæ‰‹æŒ‡","æœ‰åŠ©äºé˜²æ­¢è‚¿èƒ€"
]
NEG_LABELS = [
    "æ²¡æœ‰ä½œç”¨/æ²¡æœ‰æ•ˆæœ","ç¼çº¿è£‚å¼€","äºŒæ‰‹å•†å“","è´¨é‡é—®é¢˜","ä¸é€‚åˆ","å°ºç å¤ªå°","å°ºç ä¸å¯¹","æ¥ç¼å¤„ä¸èˆ’é€‚","ä¸è€ç”¨",
    "å°ºç å¤ªå¤§","è¿‡æ•","å…‰æ»‘/æ²¡æœ‰æŠ“æ¡","å®ç‰©ä¸è´­ä¹°æ•°é‡ä¸ä¸€è‡´"
]

# å…œåº•ï¼ˆå¿…é¡»åœ¨åº“é‡Œï¼‰
POS_FALLBACK = "èˆ’é€‚è´´åˆ"
NEG_FALLBACK = "ä¸é€‚åˆ"

# =========================
# 2) æ ‡ç­¾â€œç§å­è§¦å‘è¯â€ï¼ˆç”¨äºå¼±ç›‘ç£åˆ†æ¡¶ï¼‰
#    è¿™äº›ä¸æ˜¯æœ€ç»ˆå…³é”®è¯åº“ï¼Œç¨‹åºä¼šç”¨æ•°æ®å­¦ä¹ å¹¶æ‰©å±•æƒé‡
#    ä½ å¯ä»¥åç»­ç»§ç»­åŠ /æ”¹ï¼ˆè¶Šè´´è¿‘ä½ å“ç±»è¶Šå‡†ï¼‰
# =========================
SEEDS_POS = {
    "é¢æ–™èˆ’é€‚": ["comfortable", "soft", "èˆ’æœ", "æŸ”è½¯"],
    "è´¨é‡å¾ˆå¥½": ["well made", "good quality", "è´¨é‡", "åšå·¥å¥½"],
    "æœ‰åŠ©äºé”»ç‚¼": ["workout", "exercise", "gym", "é”»ç‚¼"],
    "æœ‰åŠ©äºç¼“è§£ç–¼ç—›": ["pain relief", "relief pain", "ç–¼ç—›", "ç¼“è§£"],
    "ä¿æš–": ["warm", "keep warm", "ä¿æš–"],
    "èˆ’é€‚è´´åˆ": ["fits well", "snug", "è´´åˆ", "åˆé€‚"],
    "æœ‰å‹ç¼©æ„Ÿ": ["compression", "compressive", "å‹ç¼©"],
    "æŠ“æ¡å¼æœ‰æ•ˆ": ["grip", "grippy", "æŠ“æ¡", "é˜²æ»‘"],
    "åˆèº«": ["perfect fit", "true to size", "åˆèº«", "åˆšå¥½"],
    "æœ‰åŠ©äºå…³èŠ‚ç‚/æ‰³æœºæŒ‡": ["arthritis", "trigger finger", "å…³èŠ‚ç‚", "æ‰³æœºæŒ‡"],
    "å¢åŠ æ‰‹æŒ‡çµæ´»": ["flexible", "dexterity", "çµæ´»"],
    "ä¿ƒè¿›è¡€æ¶²å¾ªç¯": ["circulation", "blood flow", "è¡€æ¶²å¾ªç¯"],
    "è€ç”¨": ["durable", "last long", "è€ç”¨"],
    "ç¼“è§£ä¸é€‚": ["relieve", "help", "ä¸é€‚", "ç¼“è§£"],
    "è½»ç›ˆ": ["lightweight", "light", "è½»", "è½»ç›ˆ"],
    "è¦†ç›–æ•´ä¸ªæ‰‹æŒ‡": ["full finger", "full fingers", "è¦†ç›–", "å…¨æŒ‡"],
    "æœ‰åŠ©äºé˜²æ­¢è‚¿èƒ€": ["swelling", "prevent swelling", "è‚¿èƒ€", "é˜²æ­¢è‚¿èƒ€"],
}

SEEDS_NEG = {
    "æ²¡æœ‰ä½œç”¨/æ²¡æœ‰æ•ˆæœ": ["no effect", "doesn't work", "no help", "æ²¡ç”¨", "æ²¡æœ‰æ•ˆæœ"],
    "ç¼çº¿è£‚å¼€": ["seam", "stitch", "split", "ç¼çº¿", "å¼€çº¿"],
    "äºŒæ‰‹å•†å“": ["used", "second hand", "äºŒæ‰‹", "ç”¨è¿‡"],
    "è´¨é‡é—®é¢˜": ["poor quality", "cheap", "quality issue", "è´¨é‡é—®é¢˜", "å·®"],
    "ä¸é€‚åˆ": ["not fit", "not suitable", "ä¸é€‚åˆ"],
    "å°ºç å¤ªå°": ["too small", "runs small", "tight", "å¤ªå°", "åå°"],
    "å°ºç ä¸å¯¹": ["wrong size", "size not right", "å°ºç ä¸å¯¹", "ä¹°é”™å°ºç "],
    "æ¥ç¼å¤„ä¸èˆ’é€‚": ["seam hurts", "seam uncomfortable", "æ¥ç¼", "ç£¨", "ç¡Œ"],
    "ä¸è€ç”¨": ["not durable", "broke", "tear", "æ˜“ç ´", "ä¸è€ç”¨"],
    "å°ºç å¤ªå¤§": ["too big", "runs large", "loose", "å¤ªå¤§", "åå¤§"],
    "è¿‡æ•": ["allergy", "rash", "red", "è¿‡æ•", "çº¢è‚¿"],
    "å…‰æ»‘/æ²¡æœ‰æŠ“æ¡": ["slippery", "no grip", "æ»‘", "æ²¡æœ‰æŠ“æ¡"],
    "å®ç‰©ä¸è´­ä¹°æ•°é‡ä¸ä¸€è‡´": ["missing", "quantity", "not enough", "æ•°é‡", "å°‘äº†", "ç¼º"],
}

# =========================
# 3) æ–‡ä»¶è¯»å– & åˆ—è‡ªåŠ¨è¯†åˆ«
# =========================
def load_file(f):
    name = f.name.lower()
    if name.endswith(".csv"):
        try:
            return pd.read_csv(f, encoding="utf-8")
        except UnicodeDecodeError:
            return pd.read_csv(f, encoding="gbk")
    return pd.read_excel(f)

def parse_rating(x):
    if pd.isna(x):
        return np.nan
    m = re.search(r"(\d+(?:\.\d+)?)", str(x))
    return float(m.group(1)) if m else np.nan

def auto_match_column(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    for cand in candidates:
        cl = cand.lower()
        for col in cols:
            if cl in col.lower():
                return col
    return None

COLUMN_CANDIDATES = {
    "rating": ["æ˜Ÿçº§","rating","Rating","score","Score","è¯„åˆ†"],
    "title": ["æ ‡é¢˜","title","Title","headline","summary"],
    "content": ["å†…å®¹(ç¿»è¯‘)","å†…å®¹ï¼ˆç¿»è¯‘ï¼‰","ç¿»è¯‘","translation","Translated","å†…å®¹","content","Content","review","Review","è¯„è®ºå†…å®¹","text","body"],
    "date": ["è¯„è®ºæ—¶é—´","date","Date","review_date","time","æ—¶é—´","è¯„è®ºæ—¥æœŸ"],
}

def build_text(row, col_title, col_text):
    t = str(row.get(col_text, "") or "")
    if col_title:
        h = str(row.get(col_title, "") or "")
        if h.strip():
            return f"{h.strip()} | {t.strip()}"
    return t.strip()

# =========================
# 4) Tokenizeï¼šè‹±æ–‡è¯ + bigram + ä¸­æ–‡2-gramï¼ˆæ— éœ€ jiebaï¼‰
# =========================
def tokenize_mixed(text: str):
    if not text:
        return []
    s = text.lower()

    # è‹±æ–‡è¯
    eng = re.findall(r"[a-z]+", s)
    eng_bi = [f"{eng[i]} {eng[i+1]}" for i in range(len(eng)-1)]

    # ä¸­æ–‡ï¼šæå–è¿ç»­ä¸­æ–‡å¹¶åš2-gram
    zh_chunks = re.findall(r"[\u4e00-\u9fff]+", s)
    zh_tokens = []
    for chunk in zh_chunks:
        if len(chunk) == 1:
            zh_tokens.append(chunk)
        else:
            zh_tokens.extend([chunk[i:i+2] for i in range(len(chunk)-1)])

    return eng + eng_bi + zh_tokens

# =========================
# 5) ä»æ•°æ®å­¦ä¹ ï¼štoken ææ€§æƒé‡ï¼ˆ1â€“3 vs 5ï¼‰
# =========================
def learn_polarity_weights(texts, ratings, min_df=3):
    neg = Counter()
    pos = Counter()
    for txt, r in zip(texts, ratings):
        toks = tokenize_mixed(txt)
        if r <= 3:
            neg.update(toks)
        elif r == 5:
            pos.update(toks)

    vocab = set(neg) | set(pos)
    weights = {}
    for t in vocab:
        fn, fp = neg[t], pos[t]
        if fn + fp < min_df:
            continue
        # log-oddsï¼š>0 æ›´åƒå·®è¯„ï¼Œ<0 æ›´åƒå¥½è¯„
        w = math.log((fn + 1) / (fp + 1))
        weights[t] = w
    return weights, neg, pos

# =========================
# 6) å¼±ç›‘ç£åˆ†æ¡¶ï¼šç”¨ seed è§¦å‘æŠŠéƒ¨åˆ†è¯„è®ºåˆ†åˆ°å„æ ‡ç­¾æ¡¶
# =========================
def weak_assign_bucket(text, seeds_map):
    s = text.lower()
    hit_labels = []
    for label, seeds in seeds_map.items():
        for kw in seeds:
            if kw.lower() in s:
                hit_labels.append(label)
                break
    return hit_labels

def learn_label_keyword_weights(df, polarity_weights, seeds_pos, seeds_neg, min_df_label=2, topk=40):
    """
    è¾“å‡ºï¼šlabel_kw[label][token] = weight
    weight = (token åœ¨ label æ¡¶çš„ç›¸å¯¹å¼ºåº¦) * (token çš„ææ€§å¼ºåº¦)
    """
    label_docs = defaultdict(list)  # label -> list of token lists

    for _, row in df.iterrows():
        r = int(row["rating"])
        text = row["text"]
        toks = tokenize_mixed(text)

        if r <= 3:
            hits = weak_assign_bucket(text, seeds_neg)
            for lb in hits:
                label_docs[lb].append(toks)
        elif r == 5:
            hits = weak_assign_bucket(text, seeds_pos)
            for lb in hits:
                label_docs[lb].append(toks)
        else:
            # 4æ˜Ÿä¸å‚ä¸å­¦ä¹ ï¼Œé¿å…æ··æ‚ï¼ˆåªç”¨äºæ¨ç†ï¼‰
            pass

    # ç»Ÿè®¡æ¯ä¸ª label æ¡¶å†… token freq
    label_kw = {}
    for label, docs in label_docs.items():
        c = Counter()
        for toks in docs:
            c.update(toks)

        # è®¡ç®— token æƒé‡ï¼ˆæ¡¶å†…ç›¸å¯¹ + ææ€§ï¼‰
        total = sum(c.values()) + 1e-9
        token_scores = {}
        for t, f in c.items():
            if f < min_df_label:
                continue
            pol = polarity_weights.get(t, 0.0)
            # æ¡¶å†…ç›¸å¯¹é¢‘ç‡ï¼ˆé¿å…å…¨æ˜¯é«˜é¢‘è™šè¯ï¼‰
            rel = f / total
            # æœ€ç»ˆæƒé‡ï¼šç›¸å¯¹é¢‘ç‡ * |polarity|ï¼Œå¹¶å¯¹æ–¹å‘åšä¸€è‡´æ€§çº¦æŸ
            # å·®è¯„æ ‡ç­¾å¸Œæœ› pol>0ï¼Œå¥½è¯„æ ‡ç­¾å¸Œæœ› pol<0
            if label in NEG_LABELS and pol <= 0:
                continue
            if label in POS_LABELS and pol >= 0:
                continue
            token_scores[t] = rel * abs(pol)

        # åªä¿ç•™ TopK
        top = dict(sorted(token_scores.items(), key=lambda x: x[1], reverse=True)[:topk])
        label_kw[label] = top

    # å¯¹äºæ²¡æœ‰å­¦åˆ°çš„ labelï¼ˆæ ·æœ¬å¤ªå°‘ï¼‰ï¼Œç»™ç©ºå­—å…¸ï¼ˆæ¨ç†æ—¶é  fallbackï¼‰
    for lb in POS_LABELS:
        label_kw.setdefault(lb, {})
    for lb in NEG_LABELS:
        label_kw.setdefault(lb, {})

    return label_kw

# =========================
# 7) æ¨ç†ï¼šå¯¹æ¯æ¡è¯„è®ºæŒ‰æ˜Ÿçº§é€‰ labelï¼ˆ100%è¦†ç›–ï¼‰
# =========================
def score_with_label_kw(toks, label_kw):
    s = 0.0
    for t in toks:
        if t in label_kw:
            s += label_kw[t]
    return s

def choose_label(row, label_kw, mode):
    """
    mode: 'neg_only' / 'pos_only' / 'four_star'
    """
    text = row["text"]
    toks = tokenize_mixed(text)

    if mode == "neg_only":
        best_lb, best_sc = None, -1e18
        for lb in NEG_LABELS:
            sc = score_with_label_kw(toks, label_kw.get(lb, {}))
            if sc > best_sc:
                best_lb, best_sc = lb, sc
        return best_lb if best_sc > 0 else NEG_FALLBACK

    if mode == "pos_only":
        best_lb, best_sc = None, -1e18
        for lb in POS_LABELS:
            sc = score_with_label_kw(toks, label_kw.get(lb, {}))
            if sc > best_sc:
                best_lb, best_sc = lb, sc
        return best_lb if best_sc > 0 else POS_FALLBACK

    # 4æ˜Ÿï¼šä¼˜å…ˆå·®è¯„
    best_neg, sc_neg = None, -1e18
    for lb in NEG_LABELS:
        sc = score_with_label_kw(toks, label_kw.get(lb, {}))
        if sc > sc_neg:
            best_neg, sc_neg = lb, sc
    if sc_neg > 0:
        return best_neg

    best_pos, sc_pos = None, -1e18
    for lb in POS_LABELS:
        sc = score_with_label_kw(toks, label_kw.get(lb, {}))
        if sc > sc_pos:
            best_pos, sc_pos = lb, sc
    return best_pos if sc_pos > 0 else POS_FALLBACK

# =========================
# 8) UI
# =========================
st.title("ğŸ·ï¸ è¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆä»Excelå­¦ä¹ å…³é”®è¯æƒé‡ / æ— æ¨¡å‹ / 100%è¦†ç›–ï¼‰")
st.caption("ä¸Šä¼  â†’ ç³»ç»Ÿè‡ªåŠ¨å­¦ä¹ ã€Œå…³é”®è¯â†’æ ‡ç­¾æƒé‡ã€â†’ å…¨é‡æ‰“æ ‡ â†’ ä¸‹è½½ç»“æœï¼ˆä¸éœ€è¦å¤åˆ¶ç²˜è´´ä»»ä½•ä¸œè¥¿ï¼‰")

uploaded = st.file_uploader("ä¸Šä¼ è¯„è®ºæ–‡ä»¶ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx"])

with st.expander("æ ‡ç­¾åº“ï¼ˆåªè¯»å±•ç¤ºï¼šè¾“å‡ºåªä¼šä½¿ç”¨è¿™äº›æ ‡ç­¾ï¼‰", expanded=False):
    c1, c2 = st.columns(2)
    with c1:
        st.write("âœ… å¥½è¯„æ ‡ç­¾åº“")
        st.write(POS_LABELS)
    with c2:
        st.write("âŒ å·®è¯„æ ‡ç­¾åº“")
        st.write(NEG_LABELS)

if uploaded:
    df_raw = load_file(uploaded)

    cols = df_raw.columns.tolist()
    col_rating = auto_match_column(cols, COLUMN_CANDIDATES["rating"])
    col_title = auto_match_column(cols, COLUMN_CANDIDATES["title"])
    col_text = auto_match_column(cols, COLUMN_CANDIDATES["content"])
    col_date = auto_match_column(cols, COLUMN_CANDIDATES["date"])

    if not col_rating or not col_text:
        st.error("âŒ æ— æ³•è‡ªåŠ¨è¯†åˆ«ã€æ˜Ÿçº§ã€‘æˆ–ã€å†…å®¹/ç¿»è¯‘ã€‘åˆ—ã€‚è¯·æ£€æŸ¥è¡¨å¤´å‘½åã€‚")
        st.write({"rating": col_rating, "title": col_title, "text": col_text, "date": col_date})
        st.stop()

    df = df_raw.copy()
    df["rating_raw"] = df[col_rating]
    df["rating"] = df["rating_raw"].apply(parse_rating)
    df = df.dropna(subset=["rating"])
    df["rating"] = df["rating"].round().astype(int)
    df = df[df["rating"].between(1, 5)]

    df["text"] = df.apply(lambda r: build_text(r, col_title, col_text), axis=1)

    raw_total = len(df_raw)
    valid_total = len(df)
    invalid_total = raw_total - valid_total

    neg_rate = (df["rating"] <= 3).mean() * 100 if valid_total else 0.0
    severe_rate = (df["rating"] <= 2).mean() * 100 if valid_total else 0.0

    st.subheader("ğŸ“Š è‡ªåŠ¨çœ‹æ¿")
    k1, k2, k3, k4, k5 = st.columns(5)
    k1.metric("åŸå§‹è¡Œæ•°", raw_total)
    k2.metric("æœ‰æ•ˆè¯„åˆ†è¡Œæ•°", valid_total)
    k3.metric("è§£æå¤±è´¥/æ— æ•ˆè¡Œ", invalid_total)
    k4.metric("å·®è¯„å æ¯”(â‰¤3â­)", f"{neg_rate:.1f}%")
    k5.metric("ä¸¥é‡å·®è¯„(â‰¤2â­)", f"{severe_rate:.1f}%")

    dist = df["rating"].value_counts().reindex([1,2,3,4,5], fill_value=0).sort_index()
    st.bar_chart(dist)

    st.markdown("---")
    st.subheader("ğŸ§  Step Aï¼šä»æ•°æ®å­¦ä¹  token ææ€§æƒé‡ï¼ˆâ‰¤3 vs 5ï¼‰")
    min_df = st.slider("ææ€§å­¦ä¹ ï¼štokenæœ€å°å‡ºç°æ¬¡æ•°(min_df)", 1, 10, 3, 1)
    polarity_weights, neg_counter, pos_counter = learn_polarity_weights(df["text"].tolist(), df["rating"].tolist(), min_df=min_df)
    st.success(f"å·²å­¦ä¹ ææ€§æƒé‡ï¼š{len(polarity_weights)} ä¸ª token")

    st.subheader("ğŸ§© Step Bï¼šå¼±ç›‘ç£åˆ†æ¡¶ + å­¦ä¹ ã€Œå…³é”®è¯â†’æ ‡ç­¾ã€æƒé‡")
    topk = st.slider("æ¯ä¸ªæ ‡ç­¾ä¿ç•™ TopK å…³é”®è¯", 10, 120, 40, 5)
    min_df_label = st.slider("æ ‡ç­¾æ¡¶å†…ï¼štokenæœ€å°å‡ºç°æ¬¡æ•°", 1, 8, 2, 1)

    label_kw = learn_label_keyword_weights(
        df,
        polarity_weights=polarity_weights,
        seeds_pos=SEEDS_POS,
        seeds_neg=SEEDS_NEG,
        min_df_label=min_df_label,
        topk=topk
    )
    st.success("å·²å­¦ä¹ æ ‡ç­¾å…³é”®è¯æƒé‡ï¼ˆç”¨äºå…¨é‡æ‰“æ ‡ï¼‰")

    with st.expander("æŸ¥çœ‹ï¼šæ¯ä¸ªæ ‡ç­¾å­¦åˆ°çš„ Top å…³é”®è¯ï¼ˆå¯ç”¨äºä½ è¿­ä»£æ ‡ç­¾åº“/å†™PPTï¼‰", expanded=False):
        show_lb = st.selectbox("é€‰æ‹©ä¸€ä¸ªæ ‡ç­¾æŸ¥çœ‹å…³é”®è¯æƒé‡", POS_LABELS + NEG_LABELS, index=0)
        kv = label_kw.get(show_lb, {})
        if not kv:
            st.info("è¯¥æ ‡ç­¾åœ¨æ•°æ®ä¸­è§¦å‘æ ·æœ¬è¾ƒå°‘ï¼Œç›®å‰å­¦åˆ°çš„å…³é”®è¯è¾ƒå°‘ï¼›ä»å¯ç”¨å…œåº•/å…¶å®ƒæ ‡ç­¾è¦†ç›–ã€‚")
        else:
            tmp = pd.DataFrame({"token": list(kv.keys()), "weight": list(kv.values())}).sort_values("weight", ascending=False)
            st.dataframe(tmp, use_container_width=True)

    st.markdown("---")
    st.subheader("ğŸ·ï¸ Step Cï¼šå…¨é‡æ‰“æ ‡ï¼ˆ100%è¦†ç›–ï¼Œ4æ˜Ÿä¼˜å…ˆå·®è¯„ï¼‰")
    df["AI_Label"] = df.apply(
        lambda r: choose_label(
            r,
            label_kw=label_kw,
            mode="neg_only" if r["rating"] <= 3 else ("pos_only" if r["rating"] == 5 else "four_star")
        ),
        axis=1
    )

    # æ ¡éªŒï¼šç¡®ä¿100%éƒ½åœ¨åº“é‡Œ
    allowed = set(POS_LABELS + NEG_LABELS)
    bad = df[~df["AI_Label"].isin(allowed)]
    if len(bad) > 0:
        st.warning(f"å‘ç° {len(bad)} æ¡æ ‡ç­¾ä¸åœ¨åº“å†…ï¼ˆå·²è‡ªåŠ¨å›é€€å…œåº•ï¼‰")
        df.loc[~df["AI_Label"].isin(allowed), "AI_Label"] = np.where(df["rating"] <= 3, NEG_FALLBACK, POS_FALLBACK)

    st.subheader("é¢„è§ˆï¼ˆå‰ 30 æ¡ï¼‰")
    st.dataframe(df[[col_rating, "rating", "AI_Label", "text"]].head(30), use_container_width=True)

    # æ ‡ç­¾åˆ†å¸ƒ
    st.subheader("æ ‡ç­¾å æ¯”ï¼ˆTop 20ï¼‰")
    lab_dist = df["AI_Label"].value_counts().head(20)
    st.bar_chart(lab_dist)

    st.markdown("---")
    st.subheader("â¬‡ï¸ å¯¼å‡º")
    out_full = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ä¸‹è½½ï¼šå…¨é‡æ‰“æ ‡ç»“æœ CSVï¼ˆå«AI_Labelï¼‰", out_full, "tagged_reviews_weighted_labels.csv", "text/csv")

    # é¢å¤–å¯¼å‡ºï¼šå­¦ä¹ åˆ°çš„â€œå…³é”®è¯â†’æ ‡ç­¾æƒé‡è¡¨â€
    rows = []
    for lb, kv in label_kw.items():
        for t, w in kv.items():
            rows.append({"label": lb, "token": t, "weight": w})
    kw_df = pd.DataFrame(rows).sort_values(["label", "weight"], ascending=[True, False])
    out_kw = kw_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ä¸‹è½½ï¼šå­¦ä¹ åˆ°çš„å…³é”®è¯æƒé‡è¡¨ï¼ˆlabel-token-weightï¼‰", out_kw, "label_keyword_weights.csv", "text/csv")
