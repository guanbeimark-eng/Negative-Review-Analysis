import streamlit as st
import pandas as pd
import json
import uuid
import re
import numpy as np
from typing import List, Dict, Any, Optional

# OpenAI SDK (pip install openai)
from openai import OpenAI

# =========================
# 0) App Config + Login
# =========================
st.set_page_config(page_title="è¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆå‚»ç“œå¼ä¸€é”®ç‰ˆï¼‰", page_icon="ğŸ·ï¸", layout="wide")

ACCESS_PASSWORD = "admin123"
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def check_password():
    if st.session_state.get("password_input") == ACCESS_PASSWORD:
        st.session_state.logged_in = True
    else:
        st.error("å¯†ç é”™è¯¯")

if not st.session_state.logged_in:
    st.markdown("## ğŸ”’ ç³»ç»Ÿé”å®š")
    st.text_input("è®¿é—®å¯†ç ", type="password", key="password_input", on_change=check_password)
    st.stop()

# =========================
# 1) å†…ç½®è¯„ä»·åº“ï¼ˆé»˜è®¤ï¼‰
# ä½ åç»­æŠŠè¿™é‡Œæ›¿æ¢æˆä½ ä»¬â€œæ–‡ä»¶è¯„ä»·åº“â€çš„æ­£å¼æ ‡ç­¾å³å¯
# =========================
DEFAULT_TAG_LIBRARY = {
    "positive": [
        "ä½©æˆ´èˆ’é€‚", "æ”¯æ’‘æ€§å¥½", "ç¼“è§£å…³èŠ‚ä¸é€‚", "å°ºå¯¸åˆé€‚", "è´¨é‡å¥½", "æ€§ä»·æ¯”é«˜", "æ•ˆæœæ˜æ˜¾", "ç‰©æµ/å‘è´§å¿«", "å¤–è§‚å¥½çœ‹"
    ],
    "negative": [
        "å°ºç åå°", "å°ºç åå¤§", "å°ºç ä¸ä¸€è‡´", "ä¸é€‚åˆç”·å£«", "ç©¿æˆ´å›°éš¾", "è´¨é‡å·®", "ä¸æè¿°ä¸ç¬¦",
        "ä¸èˆ’é€‚/å‹’æ‰‹", "æ°”å‘³/å¼‚å‘³", "è€ç”¨æ€§å·®/æ˜“ç ´", "å‹åŠ›/å‹ç¼©æ„Ÿä¸è¶³"
    ]
}

# =========================
# 2) Session State
# =========================
defaults = {
    "raw_df": None,
    "main_df": None,          # æ¸…æ´—åä¸»è¡¨ï¼ˆåŸå­—æ®µ + rating_int + sys_id + __text__ï¼‰
    "norm_df": None,          # id/rating/text
    "full_df": None,          # ä¸»è¡¨+AI_Label åˆå¹¶åçš„å¯¼å‡ºè¡¨
    "col_map": None,
    "tag_config": {"pos": DEFAULT_TAG_LIBRARY["positive"],
                   "neg": DEFAULT_TAG_LIBRARY["negative"],
                   "all": DEFAULT_TAG_LIBRARY["positive"] + DEFAULT_TAG_LIBRARY["negative"]},
}
for k, v in defaults.items():
    if k not in st.session_state:
        st.session_state[k] = v

# =========================
# 3) Utils
# =========================
def load_file(f):
    name = f.name.lower()
    if name.endswith(".csv"):
        try:
            return pd.read_csv(f, encoding="utf-8")
        except UnicodeDecodeError:
            return pd.read_csv(f, encoding="gbk")
    return pd.read_excel(f)

def parse_rating(x):
    """å…¼å®¹ï¼š'4.0 out of 5 stars' / 'Rated 3' / '5' / 4.0"""
    if pd.isna(x):
        return np.nan
    s = str(x)
    m = re.search(r"(\d+(?:\.\d+)?)", s)
    if not m:
        return np.nan
    try:
        return float(m.group(1))
    except:
        return np.nan

COLUMN_CANDIDATES = {
    "rating": ["æ˜Ÿçº§", "rating", "Rating", "è¯„åˆ†", "Score"],
    "title": ["æ ‡é¢˜", "title", "Title", "headline", "summary"],
    "content": ["å†…å®¹", "content", "Content", "review", "Review", "è¯„è®ºå†…å®¹", "body", "text"],
    "translation": ["å†…å®¹(ç¿»è¯‘)", "ç¿»è¯‘", "translation", "Translated", "å†…å®¹ï¼ˆç¿»è¯‘ï¼‰"],
    "date": ["è¯„è®ºæ—¶é—´", "date", "Date", "review_date", "time", "æ—¶é—´", "è¯„è®ºæ—¥æœŸ"],
    "id": ["review_id", "id", "ID", "è¯„è®ºID", "uuid", "å”¯ä¸€ID"],
}

def auto_match_column(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    for cand in candidates:
        cl = cand.lower()
        for col in cols:
            if cl in col.lower():
                return col
    return None

def auto_build_mapping(df):
    cols = df.columns.tolist()
    col_rating = auto_match_column(cols, COLUMN_CANDIDATES["rating"])
    col_title = auto_match_column(cols, COLUMN_CANDIDATES["title"])
    col_content = auto_match_column(cols, COLUMN_CANDIDATES["content"])
    col_trans = auto_match_column(cols, COLUMN_CANDIDATES["translation"])
    col_date = auto_match_column(cols, COLUMN_CANDIDATES["date"])
    col_id = auto_match_column(cols, COLUMN_CANDIDATES["id"])

    text_primary = col_trans or col_content
    return {
        "rating": col_rating,
        "title": col_title,
        "text": text_primary,   # ä¼˜å…ˆç¿»è¯‘åˆ—
        "date": col_date,
        "id": col_id,
        "content_raw": col_content,
        "translation": col_trans
    }

def build_cleaned_frames(df_raw, m):
    tmp = df_raw.copy()

    tmp["rating_numeric"] = tmp[m["rating"]].apply(parse_rating) if m.get("rating") else np.nan
    invalid_rating_cnt = int(tmp["rating_numeric"].isna().sum())

    valid = tmp.dropna(subset=["rating_numeric"]).copy()
    valid["rating_int"] = valid["rating_numeric"].round().astype(int)
    valid = valid[valid["rating_int"].between(1, 5)]

    time_ok = False
    if m.get("date") and m["date"] in valid.columns:
        valid["date_parsed"] = pd.to_datetime(valid[m["date"]], errors="coerce")
        time_ok = valid["date_parsed"].notna().sum() > 0

    if m.get("id") and m["id"] in valid.columns:
        valid["sys_id"] = valid[m["id"]].astype(str)
    else:
        valid["sys_id"] = [str(uuid.uuid4())[:8] for _ in range(len(valid))]

    if m.get("text") is None:
        valid["__text__"] = ""
    else:
        if m.get("title") and m["title"] in valid.columns:
            valid["__text__"] = (
                valid[m["title"]].fillna("").astype(str).str.strip()
                + " | "
                + valid[m["text"]].fillna("").astype(str).str.strip()
            ).str.strip(" |")
        else:
            valid["__text__"] = valid[m["text"]].fillna("").astype(str)

    norm = valid[["sys_id", "rating_int", "__text__"]].rename(columns={
        "sys_id": "id",
        "rating_int": "rating",
        "__text__": "text"
    }).copy()

    return valid, norm, invalid_rating_cnt, time_ok

def strict_json_load(s: str) -> Optional[Any]:
    """å°½é‡ä»æ¨¡å‹è¾“å‡ºé‡ŒæŠ å‡º JSONï¼ˆæ”¯æŒåŒ…è£¹åœ¨```é‡Œã€å‰åæœ‰æ‚å­—çš„æƒ…å†µï¼‰"""
    if not s:
        return None
    s = s.strip().replace("```json", "").replace("```", "").strip()

    # ç›´æ¥å°è¯•
    try:
        return json.loads(s)
    except:
        pass

    # å°è¯•æå–ç¬¬ä¸€ä¸ª [...] æ®µ
    m = re.search(r"(\[\s*\{.*\}\s*\])", s, flags=re.DOTALL)
    if m:
        try:
            return json.loads(m.group(1))
        except:
            return None
    return None

def validate_label(label: str, allowed_set: set) -> str:
    lab = (label or "").strip()
    return lab if lab in allowed_set else ""

# =========================
# 4) OpenAI è°ƒç”¨ï¼šä¸€é”®è‡ªåŠ¨æ‰“æ ‡
# =========================
def build_api_prompt(records: List[Dict[str, Any]],
                     mode: str,
                     pos_tags: List[str],
                     neg_tags: List[str]) -> str:
    pos_str = ", ".join([f'"{t}"' for t in pos_tags])
    neg_str = ", ".join([f'"{t}"' for t in neg_tags])

    header = (
        "ä½ æ˜¯ç”µå•†å®¢æˆ·è¯„è®ºçš„æ ‡ç­¾å½’ç±»ä¸“å®¶ã€‚\n"
        "ä½ å¿…é¡»ä¸¥æ ¼åªè¾“å‡º JSON listï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
        "[{\"id\":\"...\",\"label\":\"...\"}, ...]\n"
        "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡å­—ï¼Œä¸è¦è¾“å‡º markdown ä»£ç å—ã€‚\n"
        "label å¿…é¡»ä»ç»™å®šæ ‡ç­¾åº“ä¸­é€‰æ‹©ï¼›ä¸åŒ¹é…åˆ™è¾“å‡ºç©ºå­—ç¬¦ä¸² \"\"ã€‚\n"
    )

    if mode == "1-3":
        rules = f"\nè¿™äº›æ˜¯ 1-3 æ˜Ÿè¯„è®ºï¼šåªèƒ½ä»ã€å·®è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\nå·®è¯„æ ‡ç­¾åº“ï¼š[{neg_str}]\n"
    elif mode == "5":
        rules = f"\nè¿™äº›æ˜¯ 5 æ˜Ÿè¯„è®ºï¼šåªèƒ½ä»ã€å¥½è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\nå¥½è¯„æ ‡ç­¾åº“ï¼š[{pos_str}]\n"
    else:  # 4-star
        rules = (
            f"\nè¿™äº›æ˜¯ 4 æ˜Ÿè¯„è®ºï¼šä¼˜å…ˆæ‰¾å·®è¯„ç‚¹ã€‚\n"
            "è§„åˆ™ï¼š\n"
            "1) åªè¦æœ‰ä»»ä½•æŠ±æ€¨/ä¸æ»¡æ„/ç¼ºç‚¹ï¼Œå°±ä¼˜å…ˆä»ã€å·®è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\n"
            "2) å¦‚æœå®Œå…¨æ˜¯å¤¸èµï¼Œå†ä»ã€å¥½è¯„æ ‡ç­¾åº“ã€‘é€‰æ‹©ã€‚\n"
            "3) ä¸åŒ¹é…è¾“å‡ºç©ºå­—ç¬¦ä¸²ã€‚\n"
            f"å·®è¯„æ ‡ç­¾åº“ï¼š[{neg_str}]\n"
            f"å¥½è¯„æ ‡ç­¾åº“ï¼š[{pos_str}]\n"
        )

    payload = "æ•°æ®å¦‚ä¸‹ï¼ˆJSONï¼‰ï¼š\n" + json.dumps(records, ensure_ascii=False)
    return header + rules + "\n" + payload

def call_openai_tagging(client: OpenAI,
                        model: str,
                        prompt: str,
                        max_retries: int = 2) -> List[Dict[str, str]]:
    """
    è¿”å›ï¼š[{id,label}, ...]
    å¤±è´¥ä¼šé‡è¯•ï¼ˆè®©æ¨¡å‹åªè¾“å‡º JSONï¼‰
    """
    last_text = ""
    for attempt in range(max_retries + 1):
        resp = client.responses.create(
            model=model,
            input=prompt
        )
        # SDK Quickstart: response.output_text å–æ–‡æœ¬è¾“å‡º :contentReference[oaicite:2]{index=2}
        text = getattr(resp, "output_text", "") or ""
        last_text = text

        obj = strict_json_load(text)
        if isinstance(obj, list) and all(isinstance(x, dict) and "id" in x and "label" in x for x in obj):
            # æ­£å¸¸
            return [{"id": str(x["id"]), "label": str(x.get("label", "")).strip()} for x in obj]

        # é‡è¯•ï¼šç»™æ›´å¼ºçº¦æŸ
        prompt = (
            "å†æ¬¡å¼ºè°ƒï¼šä½ åªèƒ½è¾“å‡º JSON listï¼Œä¸”æ¯ä¸ªå…ƒç´ åªå…è®¸åŒ…å« id å’Œ label ä¸¤ä¸ªé”®ã€‚\n"
            "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šï¼Œä¸è¦è¾“å‡º ```ã€‚\n\n"
            + prompt
        )

    raise ValueError(f"æ¨¡å‹è¾“å‡ºæ— æ³•è§£æä¸º [{'{id,label}'}] JSONã€‚åŸå§‹è¾“å‡ºç‰‡æ®µï¼š{last_text[:500]}")

# =========================
# 5) UIï¼šçœŸæ­£å‚»ç“œå¼ï¼ˆä¸€ä¸ªä¸»æŒ‰é’®ï¼‰
# =========================
st.title("ğŸ·ï¸ è¯„è®ºè‡ªåŠ¨æ‰“æ ‡ï¼ˆå‚»ç“œå¼ï¼šä¸Šä¼  â†’ ä¸€é”®æ‰“æ ‡ â†’ å¯¼å‡ºï¼‰")

with st.expander("â‘  é…ç½® OpenAI APIï¼ˆåªéœ€ä¸€æ¬¡ï¼‰", expanded=True):
    st.caption("å»ºè®®æŠŠ API Key é…åœ¨æœåŠ¡å™¨ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼›ä¹Ÿå¯åœ¨æ­¤ä¸´æ—¶è¾“å…¥ï¼ˆä»…æœåŠ¡ç«¯ä½¿ç”¨ï¼‰ã€‚")
    api_key = st.text_input("OpenAI API Key", type="password", value="")
    model_name = st.text_input("æ¨¡å‹ï¼ˆé»˜è®¤ gpt-5.2ï¼‰", value="gpt-5.2")

    st.caption("OpenAI æ¨èä½¿ç”¨ Responses APIã€‚:contentReference[oaicite:3]{index=3}")

uploaded = st.file_uploader("â‘¡ ä¸Šä¼ è¯„è®ºæ–‡ä»¶ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx"])

# è¯„ä»·åº“å¯é€‰ç¼–è¾‘ï¼ˆä½†ä¸å¼ºè¿«ï¼‰
with st.expander("â‘¢ è¯„ä»·åº“ï¼ˆå¯é€‰ç¼–è¾‘ï¼šé»˜è®¤å·²å†…ç½®ï¼‰", expanded=False):
    c1, c2 = st.columns(2)
    with c1:
        pos_text = st.text_area("å¥½è¯„æ ‡ç­¾ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰", value="\n".join(st.session_state.tag_config["pos"]), height=220)
    with c2:
        neg_text = st.text_area("å·®è¯„æ ‡ç­¾ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰", value="\n".join(st.session_state.tag_config["neg"]), height=220)
    if st.button("ä¿å­˜è¯„ä»·åº“"):
        pos = [x.strip() for x in pos_text.splitlines() if x.strip()]
        neg = [x.strip() for x in neg_text.splitlines() if x.strip()]
        st.session_state.tag_config = {"pos": pos, "neg": neg, "all": pos + neg}
        st.success(f"å·²ä¿å­˜ï¼šå¥½è¯„ {len(pos)} ä¸ª / å·®è¯„ {len(neg)} ä¸ª")

if uploaded:
    df_raw = load_file(uploaded)
    st.session_state.raw_df = df_raw

    m = auto_build_mapping(df_raw)
    st.session_state.col_map = m

    # å¿…è¦åˆ—æ£€æŸ¥
    if not m.get("rating") or not m.get("text"):
        st.error("âŒ è‡ªåŠ¨è¯†åˆ«å¤±è´¥ï¼šç¼ºå°‘æ˜Ÿçº§åˆ—æˆ–å†…å®¹åˆ—ã€‚è¯·æ£€æŸ¥åˆ—åï¼ˆå»ºè®®ï¼šæ˜Ÿçº§/å†…å®¹/å†…å®¹(ç¿»è¯‘)ï¼‰ã€‚")
        st.json(m)
        st.stop()

    valid, norm, invalid_cnt, time_ok = build_cleaned_frames(df_raw, m)
    st.session_state.main_df = valid
    st.session_state.norm_df = norm
    st.session_state.full_df = None

    # çœ‹æ¿ï¼ˆè‡ªåŠ¨ï¼‰
    raw_total = len(df_raw)
    valid_total = len(valid)
    neg_cnt = int((valid["rating_int"] <= 3).sum())
    neg_rate = (neg_cnt / valid_total * 100) if valid_total else 0
    severe_cnt = int((valid["rating_int"] <= 2).sum())
    severe_rate = (severe_cnt / valid_total * 100) if valid_total else 0

    st.subheader("ğŸ“Š è‡ªåŠ¨çœ‹æ¿")
    k1, k2, k3, k4, k5 = st.columns(5)
    k1.metric("åŸå§‹è¡Œæ•°", raw_total)
    k2.metric("æœ‰æ•ˆè¯„åˆ†è¡Œæ•°", valid_total)
    k3.metric("è¯„åˆ†è§£æå¤±è´¥", invalid_cnt)
    k4.metric("å·®è¯„å æ¯”(â‰¤3â­)", f"{neg_rate:.1f}%")
    k5.metric("ä¸¥é‡å·®è¯„(â‰¤2â­)", f"{severe_rate:.1f}%")

    st.bar_chart(valid["rating_int"].value_counts().reindex([1,2,3,4,5], fill_value=0).sort_index())

    st.dataframe(norm.head(8))

    # ä¸€é”®æ‰“æ ‡æŒ‰é’®ï¼ˆå”¯ä¸€ä¸»æŒ‰é’®ï¼‰
    st.markdown("---")
    st.subheader("â‘£ ä¸€é”®è‡ªåŠ¨æ‰“æ ‡ï¼ˆä¸éœ€è¦å¤åˆ¶ç²˜è´´ä»»ä½•ä¸œè¥¿ï¼‰")

    batch_size = st.slider("æ¯æ‰¹æ¡æ•°ï¼ˆè¶Šå¤§è¶Šå¿«ï¼Œä½†æ›´åƒä¸Šä¸‹æ–‡ï¼‰", 20, 120, 60, 10)

    if st.button("ğŸš€ ä¸€é”®è‡ªåŠ¨æ‰“æ ‡å¹¶ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶", type="primary"):
        if not api_key:
            st.error("è¯·å…ˆå¡«å†™ OpenAI API Keyï¼ˆæˆ–åœ¨æœåŠ¡å™¨è®¾ç½® OPENAI_API_KEYï¼‰ã€‚")
            st.stop()

        client = OpenAI(api_key=api_key)

        df = st.session_state.norm_df.copy()
        pos_tags = st.session_state.tag_config["pos"]
        neg_tags = st.session_state.tag_config["neg"]
        allowed_set = set(st.session_state.tag_config["all"])

        # åˆ†ç»„ï¼š1-3 / 4 / 5
        groups = {
            "1-3": df[df["rating"] <= 3],
            "4": df[df["rating"] == 4],
            "5": df[df["rating"] == 5],
        }

        # å‡†å¤‡è¾“å‡ºåˆ—
        if "AI_Label" not in df.columns:
            df["AI_Label"] = ""

        total_jobs = 0
        for _, g in groups.items():
            total_jobs += int(np.ceil(len(g) / batch_size)) if len(g) else 0

        progress = st.progress(0)
        job_done = 0

        # æ‰§è¡Œ
        for mode, gdf in groups.items():
            if gdf.empty:
                continue
            records = gdf.to_dict("records")

            for i in range(0, len(records), int(batch_size)):
                chunk = records[i:i+int(batch_size)]
                prompt = build_api_prompt(chunk, mode, pos_tags, neg_tags)

                # è°ƒç”¨ API
                try:
                    results = call_openai_tagging(client, model_name, prompt, max_retries=2)
                except Exception as e:
                    st.error(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆ{mode}æ˜Ÿ æ‰¹æ¬¡ {i//batch_size+1}ï¼‰ï¼š{e}")
                    st.stop()

                # å›å¡« + ä¸¥æ ¼æ ¡éªŒæ ‡ç­¾åœ¨åº“å†…
                id_map = {r["id"]: validate_label(r["label"], allowed_set) for r in results}
                mask = df["id"].isin(id_map.keys())
                df.loc[mask, "AI_Label"] = df.loc[mask, "id"].map(id_map).fillna(df.loc[mask, "AI_Label"])

                job_done += 1
                progress.progress(min(1.0, job_done / max(1, total_jobs)))

        st.session_state.norm_df = df

        # åˆå¹¶å›ä¸»è¡¨
        main = st.session_state.main_df.copy()
        lab = df[["id", "AI_Label"]].copy()
        main["sys_id"] = main["sys_id"].astype(str)
        lab["id"] = lab["id"].astype(str)
        merged = main.merge(lab, left_on="sys_id", right_on="id", how="left")
        merged.drop(columns=["id"], inplace=True, errors="ignore")
        st.session_state.full_df = merged

        st.success("âœ… è‡ªåŠ¨æ‰“æ ‡å®Œæˆï¼ä½ å¯ä»¥ç›´æ¥ä¸‹è½½ç»“æœã€‚")
        st.dataframe(df.head(20))

# å¯¼å‡ºåŒºï¼ˆéšæ—¶å¯ä¸‹è½½ï¼Œæ°¸è¿œä¸ç”¨å¤åˆ¶ç²˜è´´ï¼‰
st.markdown("---")
st.subheader("â‘¤ å¯¼å‡º")

if st.session_state.norm_df is not None:
    out_norm = st.session_state.norm_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ä¸‹è½½ï¼šnormalizedï¼ˆid/rating/text/AI_Labelï¼‰",
                       out_norm, "tagged_reviews_normalized.csv", "text/csv")

if st.session_state.full_df is not None:
    out_full = st.session_state.full_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ä¸‹è½½ï¼šfullï¼ˆåŸå§‹å­—æ®µ + AI_Labelï¼‰",
                       out_full, "tagged_reviews_full.csv", "text/csv")
